{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 10539276,
          "sourceType": "datasetVersion",
          "datasetId": 6521104
        },
        {
          "sourceId": 10620870,
          "sourceType": "datasetVersion",
          "datasetId": 6576126
        },
        {
          "sourceId": 10621116,
          "sourceType": "datasetVersion",
          "datasetId": 6576269
        }
      ],
      "dockerImageVersionId": 30839,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "ML, DL, Transformer",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n"
      ],
      "metadata": {
        "id": "d8r2bfUSJaUd"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bZJaahMlJaUg"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "SnQ-W7NCJaUi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model for Malayalam"
      ],
      "metadata": {
        "id": "_zDQx8vMJaUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Load datasets\n",
        "malayalam_train = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/mal_training_data_hum_ai.csv')\n",
        "malayalam_test = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/mal_test.xlsx - Sheet1.csv')\n",
        "\n",
        "# Preprocess data with dynamic column name handling\n",
        "def preprocess(df):\n",
        "    if 'LABEL' in df.columns:\n",
        "        label_column = 'LABEL'\n",
        "    elif 'Label' in df.columns:\n",
        "        label_column = 'Label'\n",
        "    else:\n",
        "        raise KeyError(\"Label column not found. Available columns: \" + \", \".join(df.columns))\n",
        "\n",
        "    df['Label'] = df[label_column].str.lower()  # Adjust column name dynamically\n",
        "    df['Data'] = df['DATA'].str.strip()  # Adjust as per your dataset's actual data column name\n",
        "    return df[['Data', 'Label']]\n",
        "\n",
        "malayalam_train = preprocess(malayalam_train)\n",
        "malayalam_test = preprocess(malayalam_test)\n",
        "\n",
        "# Vectorize text data\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train = vectorizer.fit_transform(malayalam_train['Data'])\n",
        "X_test = vectorizer.transform(malayalam_test['Data'])\n",
        "\n",
        "# Encode labels\n",
        "encoder = LabelEncoder()\n",
        "y_train = encoder.fit_transform(malayalam_train['Label'])\n",
        "y_test = encoder.transform(malayalam_test['Label'])\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"SVM\": SVC(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Kernel SVM\": SVC(kernel='rbf'),  # Using RBF kernel for non-linear SVM\n",
        "    \"SGD\": SGDClassifier()\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy, precision, recall, and F1 score\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()  # Extract true negatives, false positives, false negatives, true positives\n",
        "\n",
        "    # Calculate specificity\n",
        "    specificity = tn / (tn + fp)\n",
        "\n",
        "    # Calculate G-mean\n",
        "    g_mean = np.sqrt(recall * specificity)\n",
        "\n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision (Macro)\": precision,\n",
        "        \"Recall (Macro)\": recall,\n",
        "        \"Macro F1 Score\": f1,\n",
        "        \"G-mean\": g_mean\n",
        "    }\n",
        "\n",
        "# Output results\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"{model_name}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.2f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T20:52:54.523215Z",
          "iopub.execute_input": "2025-01-30T20:52:54.523645Z",
          "iopub.status.idle": "2025-01-30T20:52:55.915464Z",
          "shell.execute_reply.started": "2025-01-30T20:52:54.523612Z",
          "shell.execute_reply": "2025-01-30T20:52:55.914126Z"
        },
        "id": "NK2_R3msJaUl",
        "outputId": "251f6e10-7206-4a07-a031-92b1a3539ca1"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Logistic Regression:\n  Accuracy: 0.67\n  Precision (Macro): 0.67\n  Recall (Macro): 0.67\n  Macro F1 Score: 0.66\n  G-mean: 0.64\nSVM:\n  Accuracy: 0.67\n  Precision (Macro): 0.67\n  Recall (Macro): 0.67\n  Macro F1 Score: 0.66\n  G-mean: 0.63\nRandom Forest:\n  Accuracy: 0.65\n  Precision (Macro): 0.65\n  Recall (Macro): 0.65\n  Macro F1 Score: 0.65\n  G-mean: 0.61\nNaive Bayes:\n  Accuracy: 0.62\n  Precision (Macro): 0.63\n  Recall (Macro): 0.62\n  Macro F1 Score: 0.62\n  G-mean: 0.59\nDecision Tree:\n  Accuracy: 0.57\n  Precision (Macro): 0.58\n  Recall (Macro): 0.57\n  Macro F1 Score: 0.57\n  G-mean: 0.59\nKernel SVM:\n  Accuracy: 0.67\n  Precision (Macro): 0.67\n  Recall (Macro): 0.67\n  Macro F1 Score: 0.66\n  G-mean: 0.63\nSGD:\n  Accuracy: 0.69\n  Precision (Macro): 0.69\n  Recall (Macro): 0.69\n  Macro F1 Score: 0.69\n  G-mean: 0.67\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Load datasets\n",
        "malayalam_train = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/mal_training_data_hum_ai.csv')\n",
        "malayalam_test = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/mal_test.xlsx - Sheet1.csv')\n",
        "\n",
        "\n",
        "# Preprocess data with dynamic column name handling\n",
        "def preprocess(df):\n",
        "    if 'LABEL' in df.columns:\n",
        "        label_column = 'LABEL'\n",
        "    elif 'Label' in df.columns:\n",
        "        label_column = 'Label'\n",
        "    else:\n",
        "        raise KeyError(\"Label column not found. Available columns: \" + \", \".join(df.columns))\n",
        "\n",
        "    df['Label'] = df[label_column].str.lower()  # Adjust column name dynamically\n",
        "    df['Data'] = df['DATA'].str.strip()  # Adjust as per your dataset's actual data column name\n",
        "    return df[['Data', 'Label']]\n",
        "\n",
        "malayalam_train = preprocess(malayalam_train)\n",
        "malayalam_test = preprocess(malayalam_test)\n",
        "\n",
        "# Vectorize text data\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train = vectorizer.fit_transform(malayalam_train['Data'])\n",
        "X_test = vectorizer.transform(malayalam_test['Data'])\n",
        "\n",
        "# Encode labels\n",
        "encoder = LabelEncoder()\n",
        "y_train = encoder.fit_transform(malayalam_train['Label'])\n",
        "y_test = encoder.transform(malayalam_test['Label'])\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"SVM\": SVC(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Naive Bayes\": MultinomialNB()\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "    results[name] = {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1 Score\": f1\n",
        "    }\n",
        "\n",
        "# Output results\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"{model_name}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.2f}\")\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T20:53:21.491022Z",
          "iopub.execute_input": "2025-01-30T20:53:21.491387Z",
          "iopub.status.idle": "2025-01-30T20:53:22.082005Z",
          "shell.execute_reply.started": "2025-01-30T20:53:21.49136Z",
          "shell.execute_reply": "2025-01-30T20:53:22.08088Z"
        },
        "id": "6fIQ84unJaUo",
        "outputId": "bfede501-8da3-4bd1-d64f-b825455612cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Logistic Regression:\n  Accuracy: 0.67\n  Precision: 0.65\n  Recall: 0.71\n  F1 Score: 0.68\nSVM:\n  Accuracy: 0.67\n  Precision: 0.64\n  Recall: 0.74\n  F1 Score: 0.69\nRandom Forest:\n  Accuracy: 0.68\n  Precision: 0.64\n  Recall: 0.79\n  F1 Score: 0.71\nNaive Bayes:\n  Accuracy: 0.62\n  Precision: 0.61\n  Recall: 0.70\n  F1 Score: 0.65\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model for Tamil"
      ],
      "metadata": {
        "id": "JQZxbq4QJaUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Load datasets\n",
        "tamil_train = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/tam_training_data_hum_ai.csv')\n",
        "tamil_test = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/tamil-test.xlsx - Sheet1.csv')\n",
        "\n",
        "# Preprocess data with dynamic column name handling\n",
        "def preprocess(df, data_column, label_column_options):\n",
        "    # Detect and use the correct label column\n",
        "    for label_column in label_column_options:\n",
        "        if label_column in df.columns:\n",
        "            df['Label'] = df[label_column].str.lower()\n",
        "            break\n",
        "    else:\n",
        "        raise KeyError(\"None of the label columns found. Available columns: \" + \", \".join(df.columns))\n",
        "\n",
        "    # Ensure the data column exists\n",
        "    if data_column in df.columns:\n",
        "        df['Data'] = df[data_column].str.strip()\n",
        "    else:\n",
        "        raise KeyError(f\"Data column '{data_column}' not found. Available columns: \" + \", \".join(df.columns))\n",
        "\n",
        "    return df[['Data', 'Label']]\n",
        "\n",
        "# Apply preprocessing\n",
        "tamil_train = preprocess(tamil_train, 'DATA', ['LABEL', 'Label'])\n",
        "tamil_test = preprocess(tamil_test, 'Data', ['Label'])  # Adjust the data column name if different\n",
        "\n",
        "# Vectorize text data\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train = vectorizer.fit_transform(tamil_train['Data'])\n",
        "X_test = vectorizer.transform(tamil_test['Data'])\n",
        "\n",
        "# Encode labels\n",
        "encoder = LabelEncoder()\n",
        "y_train = encoder.fit_transform(tamil_train['Label'])\n",
        "y_test = encoder.transform(tamil_test['Label'])\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"SVM\": SVC(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Kernel SVM\": SVC(kernel='rbf'),  # Using RBF kernel for non-linear SVM\n",
        "    \"SGD\": SGDClassifier()\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy, precision, recall, and F1 score (macro)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()  # Extract true negatives, false positives, false negatives, true positives\n",
        "\n",
        "    # Calculate specificity\n",
        "    specificity = tn / (tn + fp)\n",
        "\n",
        "    # Calculate G-mean\n",
        "    g_mean = np.sqrt(recall * specificity)\n",
        "\n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision (Macro)\": precision,\n",
        "        \"Recall (Macro)\": recall,\n",
        "        \"Macro F1 Score\": f1,\n",
        "        \"G-mean\": g_mean\n",
        "    }\n",
        "\n",
        "# Output results\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"{model_name}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.2f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T20:56:36.422576Z",
          "iopub.execute_input": "2025-01-30T20:56:36.422956Z",
          "iopub.status.idle": "2025-01-30T20:56:36.939385Z",
          "shell.execute_reply.started": "2025-01-30T20:56:36.422928Z",
          "shell.execute_reply": "2025-01-30T20:56:36.938263Z"
        },
        "id": "s65FNuJiJaUq",
        "outputId": "ec2b5a35-1278-4e11-dee4-6e1a99280553"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Logistic Regression:\n  Accuracy: 0.84\n  Precision (Macro): 0.84\n  Recall (Macro): 0.84\n  Macro F1 Score: 0.84\n  G-mean: 0.86\nSVM:\n  Accuracy: 0.80\n  Precision (Macro): 0.80\n  Recall (Macro): 0.80\n  Macro F1 Score: 0.80\n  G-mean: 0.83\nRandom Forest:\n  Accuracy: 0.87\n  Precision (Macro): 0.87\n  Recall (Macro): 0.87\n  Macro F1 Score: 0.87\n  G-mean: 0.87\nNaive Bayes:\n  Accuracy: 0.68\n  Precision (Macro): 0.70\n  Recall (Macro): 0.69\n  Macro F1 Score: 0.68\n  G-mean: 0.75\nDecision Tree:\n  Accuracy: 0.72\n  Precision (Macro): 0.72\n  Recall (Macro): 0.72\n  Macro F1 Score: 0.72\n  G-mean: 0.75\nKernel SVM:\n  Accuracy: 0.80\n  Precision (Macro): 0.80\n  Recall (Macro): 0.80\n  Macro F1 Score: 0.80\n  G-mean: 0.83\nSGD:\n  Accuracy: 0.84\n  Precision (Macro): 0.84\n  Recall (Macro): 0.84\n  Macro F1 Score: 0.84\n  G-mean: 0.86\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Load datasets\n",
        "#mal_train = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/mal_training_data_hum_ai.csv')\n",
        "#mal_test = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/mal_test.xlsx - Sheet1.csv')\n",
        "\n",
        "tamil_train = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/tam_training_data_hum_ai.csv')\n",
        "tamil_test = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/tamil-test.xlsx - Sheet1.csv')\n",
        "\n",
        "#tamil_train = pd.read_csv('../input/final-dataset/tam_training_data_hum_ai.csv')\n",
        "#tamil_test = pd.read_excel('../input/final-dataset/tamil-test.xlsx')\n",
        "\n",
        "# Preprocess data with dynamic column name handling\n",
        "def preprocess(df, data_column, label_column_options):\n",
        "    # Detect and use the correct label column\n",
        "    for label_column in label_column_options:\n",
        "        if label_column in df.columns:\n",
        "            df['Label'] = df[label_column].str.lower()\n",
        "            break\n",
        "    else:\n",
        "        raise KeyError(\"None of the label columns found. Available columns: \" + \", \".join(df.columns))\n",
        "\n",
        "    # Ensure the data column exists\n",
        "    if data_column in df.columns:\n",
        "        df['Data'] = df[data_column].str.strip()\n",
        "    else:\n",
        "        raise KeyError(f\"Data column '{data_column}' not found. Available columns: \" + \", \".join(df.columns))\n",
        "\n",
        "    return df[['Data', 'Label']]\n",
        "\n",
        "# Apply preprocessing\n",
        "tamil_train = preprocess(tamil_train, 'DATA', ['LABEL', 'Label'])\n",
        "tamil_test = preprocess(tamil_test, 'Data', ['Label'])  # Adjust the data column name if different\n",
        "\n",
        "# Vectorize text data\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train = vectorizer.fit_transform(tamil_train['Data'])\n",
        "X_test = vectorizer.transform(tamil_test['Data'])\n",
        "\n",
        "# Encode labels\n",
        "encoder = LabelEncoder()\n",
        "y_train = encoder.fit_transform(tamil_train['Label'])\n",
        "y_test = encoder.transform(tamil_test['Label'])\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"SVM\": SVC(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Naive Bayes\": MultinomialNB()\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "    results[name] = {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1 Score\": f1\n",
        "    }\n",
        "\n",
        "# Output results\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"{model_name}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.2f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-25T15:08:32.837122Z",
          "iopub.execute_input": "2025-01-25T15:08:32.83748Z",
          "iopub.status.idle": "2025-01-25T15:08:33.30371Z",
          "shell.execute_reply.started": "2025-01-25T15:08:32.837451Z",
          "shell.execute_reply": "2025-01-25T15:08:33.302544Z"
        },
        "id": "cgtPSBrBJaUr",
        "outputId": "03ada7d9-c92e-46c8-9634-953ac618f084"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Logistic Regression:\n  Accuracy: 0.84\n  Precision: 0.88\n  Recall: 0.81\n  F1 Score: 0.84\nSVM:\n  Accuracy: 0.80\n  Precision: 0.85\n  Recall: 0.75\n  F1 Score: 0.80\nRandom Forest:\n  Accuracy: 0.86\n  Precision: 0.88\n  Recall: 0.85\n  F1 Score: 0.86\nNaive Bayes:\n  Accuracy: 0.68\n  Precision: 0.76\n  Recall: 0.56\n  F1 Score: 0.64\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-25T15:18:55.299169Z",
          "iopub.execute_input": "2025-01-25T15:18:55.299635Z",
          "iopub.status.idle": "2025-01-25T15:18:59.587628Z",
          "shell.execute_reply.started": "2025-01-25T15:18:55.299576Z",
          "shell.execute_reply": "2025-01-25T15:18:59.586014Z"
        },
        "id": "ZupQFfndJaUs",
        "outputId": "963b3a4f-16cc-409c-b551-0acd5769b511"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\nRequirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\nRequirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0,>=1.23.5->tensorflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0,>=1.23.5->tensorflow) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0,>=1.23.5->tensorflow) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.0.0,>=1.23.5->tensorflow) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.0.0,>=1.23.5->tensorflow) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalAveragePooling1D, Dense\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# Parameters setup\n",
        "vocab_size = 10000\n",
        "max_length = 200\n",
        "embedding_dim = 128\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "# Load datasets\n",
        "mal_train = pd.read_csv('/kaggle/input/final-dataset/mal_training_data_hum_ai.csv')\n",
        "mal_test = pd.read_excel('/kaggle/input/final-dataset/mal_test.xlsx')\n",
        "tam_train = pd.read_csv('/kaggle/input/final-dataset/tam_training_data_hum_ai.csv')\n",
        "tam_test = pd.read_excel('/kaggle/input/final-dataset/tamil-test.xlsx')\n",
        "\n",
        "# Update label mapping to include all variations and potential cases\n",
        "label_mapping = {'human': 0, 'ai': 1, 'Human': 0, 'AI': 1, 'HUMAN': 0, 'AI': 1}\n",
        "\n",
        "def convert_labels(df, label_col):\n",
        "    try:\n",
        "        # Normalize labels to lowercase and convert using mapping\n",
        "        converted_labels = [label_mapping[label.lower()] for label in df[label_col]]\n",
        "    except KeyError as e:\n",
        "        missing_label = str(e).strip(\"'\")\n",
        "        raise ValueError(f\"Unmapped label found: {missing_label}. Please update the label_mapping.\")\n",
        "    return np.array(converted_labels)\n",
        "\n",
        "# Apply preprocessing and convert labels\n",
        "try:\n",
        "    mal_train['LABEL'] = convert_labels(mal_train, 'LABEL')\n",
        "    mal_test['Label'] = convert_labels(mal_test, 'Label')\n",
        "    tam_train['LABEL'] = convert_labels(tam_train, 'LABEL')\n",
        "    tam_test['Label'] = convert_labels(tam_test, 'Label')\n",
        "except ValueError as e:\n",
        "    print(e)  # For debugging any further issues with labels\n",
        "\n",
        "# Combine text data for tokenization\n",
        "all_text = pd.concat([mal_train['DATA'], mal_test['DATA'], tam_train['DATA'], tam_test['Data']])\n",
        "\n",
        "# Tokenization and padding sequences\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(all_text)\n",
        "train_padded_mal = tokenizer.texts_to_sequences(mal_train['DATA'])\n",
        "train_padded_mal = pad_sequences(train_padded_mal, maxlen=max_length)\n",
        "test_padded_mal = tokenizer.texts_to_sequences(mal_test['DATA'])\n",
        "test_padded_mal = pad_sequences(test_padded_mal, maxlen=max_length)\n",
        "\n",
        "train_padded_tam = tokenizer.texts_to_sequences(tam_train['DATA'])\n",
        "train_padded_tam = pad_sequences(train_padded_tam, maxlen=max_length)\n",
        "test_padded_tam = tokenizer.texts_to_sequences(tam_test['Data'])\n",
        "test_padded_tam = pad_sequences(test_padded_tam, maxlen=max_length)\n",
        "\n",
        "# Define and compile a simple model\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        Conv1D(128, 5, activation='relu'),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train and evaluate the model\n",
        "def train_and_evaluate(train_padded, train_labels, test_padded, test_labels):\n",
        "    model = create_model()\n",
        "    model.fit(train_padded, train_labels, epochs=10, verbose=2)\n",
        "    predictions = (model.predict(test_padded) > 0.5).astype(int)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(test_labels, predictions, average='binary')\n",
        "    accuracy = accuracy_score(test_labels, predictions)\n",
        "    return precision, recall, f1, accuracy\n",
        "\n",
        "# Evaluate models for Malayalam\n",
        "precision_mal, recall_mal, f1_mal, accuracy_mal = train_and_evaluate(train_padded_mal, mal_train['LABEL'], test_padded_mal, mal_test['Label'])\n",
        "print(f\"Malayalam - Precision: {precision_mal}, Recall: {recall_mal}, F1: {f1_mal}, Accuracy: {accuracy_mal}\")\n",
        "\n",
        "# Evaluate models for Tamil\n",
        "precision_tam, recall_tam, f1_tam, accuracy_tam = train_and_evaluate(train_padded_tam, tam_train['LABEL'], test_padded_tam, tam_test['Label'])\n",
        "print(f\"Tamil - Precision: {precision_tam}, Recall: {recall_tam}, F1: {f1_tam}, Accuracy: {accuracy_tam}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-25T18:31:22.702599Z",
          "iopub.execute_input": "2025-01-25T18:31:22.703063Z",
          "iopub.status.idle": "2025-01-25T18:32:04.069425Z",
          "shell.execute_reply.started": "2025-01-25T18:31:22.70303Z",
          "shell.execute_reply": "2025-01-25T18:32:04.06807Z"
        },
        "id": "u5rJDFYPJaUt",
        "outputId": "d07ada73-f75e-4cd4-960b-cb6a35d262d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/10\n25/25 - 3s - 110ms/step - accuracy: 0.5312 - loss: 0.6928\nEpoch 2/10\n25/25 - 1s - 39ms/step - accuracy: 0.5663 - loss: 0.6911\nEpoch 3/10\n25/25 - 1s - 39ms/step - accuracy: 0.7412 - loss: 0.6484\nEpoch 4/10\n25/25 - 1s - 39ms/step - accuracy: 0.7600 - loss: 0.5464\nEpoch 5/10\n25/25 - 1s - 39ms/step - accuracy: 0.8500 - loss: 0.3968\nEpoch 6/10\n25/25 - 1s - 51ms/step - accuracy: 0.8963 - loss: 0.2905\nEpoch 7/10\n25/25 - 1s - 39ms/step - accuracy: 0.9325 - loss: 0.1983\nEpoch 8/10\n25/25 - 1s - 40ms/step - accuracy: 0.9550 - loss: 0.1448\nEpoch 9/10\n25/25 - 1s - 39ms/step - accuracy: 0.9712 - loss: 0.1148\nEpoch 10/10\n25/25 - 1s - 39ms/step - accuracy: 0.9750 - loss: 0.0941\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\nMalayalam - Precision: 0.7582417582417582, Recall: 0.69, F1: 0.7225130890052356, Accuracy: 0.735\nEpoch 1/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "26/26 - 3s - 97ms/step - accuracy: 0.4988 - loss: 0.6949\nEpoch 2/10\n26/26 - 1s - 38ms/step - accuracy: 0.4765 - loss: 0.6959\nEpoch 3/10\n26/26 - 1s - 38ms/step - accuracy: 0.4641 - loss: 0.6945\nEpoch 4/10\n26/26 - 1s - 39ms/step - accuracy: 0.7178 - loss: 0.6864\nEpoch 5/10\n26/26 - 1s - 38ms/step - accuracy: 0.7426 - loss: 0.6469\nEpoch 6/10\n26/26 - 1s - 39ms/step - accuracy: 0.8589 - loss: 0.4972\nEpoch 7/10\n26/26 - 1s - 39ms/step - accuracy: 0.8676 - loss: 0.2985\nEpoch 8/10\n26/26 - 1s - 39ms/step - accuracy: 0.9233 - loss: 0.1900\nEpoch 9/10\n26/26 - 1s - 41ms/step - accuracy: 0.7673 - loss: 0.7899\nEpoch 10/10\n26/26 - 1s - 53ms/step - accuracy: 0.9455 - loss: 0.2096\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\nTamil - Precision: 0.88, Recall: 0.9166666666666666, F1: 0.8979591836734694, Accuracy: 0.9\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DL for mala tam"
      ],
      "metadata": {
        "id": "hkjjWlKPJaUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalAveragePooling1D, Dense, LSTM, Bidirectional, GRU\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# Parameters\n",
        "vocab_size = 10000\n",
        "max_length = 200\n",
        "embedding_dim = 128\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "# Load datasets\n",
        "mal_train = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/mal_training_data_hum_ai.csv')\n",
        "mal_test = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/mal_test.xlsx - Sheet1.csv')\n",
        "tam_train = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/tam_training_data_hum_ai.csv')\n",
        "tam_test = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/tamil-test.xlsx - Sheet1.csv')\n",
        "\n",
        "# Normalize labels\n",
        "label_mapping = {'human': 0, 'ai': 1}\n",
        "def convert_labels(labels):\n",
        "    return np.array([label_mapping[label.lower()] for label in labels])\n",
        "\n",
        "mal_train_labels = convert_labels(mal_train['LABEL'])\n",
        "mal_test_labels = convert_labels(mal_test['Label'])\n",
        "tam_train_labels = convert_labels(tam_train['LABEL'])\n",
        "tam_test_labels = convert_labels(tam_test['Label'])\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(pd.concat([mal_train['DATA'], tam_train['DATA']]))  # Combine text for tokenizing\n",
        "mal_train_seq = pad_sequences(tokenizer.texts_to_sequences(mal_train['DATA']), maxlen=max_length)\n",
        "mal_test_seq = pad_sequences(tokenizer.texts_to_sequences(mal_test['DATA']), maxlen=max_length)\n",
        "tam_train_seq = pad_sequences(tokenizer.texts_to_sequences(tam_train['DATA']), maxlen=max_length)\n",
        "tam_test_seq = pad_sequences(tokenizer.texts_to_sequences(tam_test['Data']), maxlen=max_length)\n",
        "\n",
        "# Model definitions\n",
        "def create_cnn_model():\n",
        "    return Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        Conv1D(128, 5, activation='relu'),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "def create_gru_model():\n",
        "    return Sequential([\n",
        "        Embedding(vocab_size, embedding_dim),\n",
        "        GRU(128),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "def create_cnn_lstm_model():\n",
        "    return Sequential([\n",
        "        Embedding(vocab_size, embedding_dim),\n",
        "        Conv1D(64, 5, activation='relu'),\n",
        "        LSTM(64),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "def create_cnn_bilstm_model():\n",
        "    return Sequential([\n",
        "        Embedding(vocab_size, embedding_dim),\n",
        "        Conv1D(64, 5, activation='relu'),\n",
        "        Bidirectional(LSTM(64)),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "# Train and evaluate\n",
        "def train_and_evaluate(model, train_data, train_labels, test_data, test_labels, language, model_name):\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.fit(train_data, train_labels, epochs=10, verbose=2)\n",
        "    predictions = (model.predict(test_data) > 0.5).astype(int)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(test_labels, predictions, average='binary')\n",
        "    accuracy = accuracy_score(test_labels, predictions)\n",
        "    print(f\"{language} - {model_name} - Precision: {precision}, Recall: {recall}, F1: {f1}, Accuracy: {accuracy}\")\n",
        "\n",
        "# Instantiate and evaluate models\n",
        "models = {\n",
        "    'CNN': create_cnn_model(),\n",
        "    'GRU': create_gru_model(),\n",
        "    'CNN-LSTM': create_cnn_lstm_model(),\n",
        "    'CNN-BiLSTM': create_cnn_bilstm_model()\n",
        "}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nEvaluating {model_name} model for Malayalam:\")\n",
        "    train_and_evaluate(model, mal_train_seq, mal_train_labels, mal_test_seq, mal_test_labels, \"Malayalam\", model_name)\n",
        "    print(f\"\\nEvaluating {model_name} model for Tamil:\")\n",
        "    train_and_evaluate(model, tam_train_seq, tam_train_labels, tam_test_seq, tam_test_labels, \"Tamil\", model_name)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T19:43:02.996587Z",
          "iopub.execute_input": "2025-01-30T19:43:02.997015Z",
          "iopub.status.idle": "2025-01-30T19:48:21.976587Z",
          "shell.execute_reply.started": "2025-01-30T19:43:02.996982Z",
          "shell.execute_reply": "2025-01-30T19:48:21.975286Z"
        },
        "id": "lJ4yqgJlJaUv",
        "outputId": "6c18e95c-cd80-41b9-c1dc-30fd37b5fdf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nEvaluating CNN model for Malayalam:\nEpoch 1/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "25/25 - 3s - 125ms/step - accuracy: 0.5200 - loss: 0.6938\nEpoch 2/10\n25/25 - 1s - 42ms/step - accuracy: 0.5238 - loss: 0.6932\nEpoch 3/10\n25/25 - 1s - 43ms/step - accuracy: 0.5987 - loss: 0.6865\nEpoch 4/10\n25/25 - 1s - 43ms/step - accuracy: 0.7987 - loss: 0.6326\nEpoch 5/10\n25/25 - 1s - 46ms/step - accuracy: 0.8612 - loss: 0.4601\nEpoch 6/10\n25/25 - 1s - 45ms/step - accuracy: 0.9013 - loss: 0.2989\nEpoch 7/10\n25/25 - 1s - 43ms/step - accuracy: 0.9150 - loss: 0.2237\nEpoch 8/10\n25/25 - 1s - 45ms/step - accuracy: 0.9513 - loss: 0.1473\nEpoch 9/10\n25/25 - 1s - 45ms/step - accuracy: 0.9762 - loss: 0.1037\nEpoch 10/10\n25/25 - 1s - 44ms/step - accuracy: 0.9737 - loss: 0.0759\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\nMalayalam - CNN - Precision: 0.7117117117117117, Recall: 0.79, F1: 0.7488151658767773, Accuracy: 0.735\n\nEvaluating CNN model for Tamil:\nEpoch 1/10\n26/26 - 3s - 102ms/step - accuracy: 0.6423 - loss: 0.7152\nEpoch 2/10\n26/26 - 1s - 47ms/step - accuracy: 0.8181 - loss: 0.5258\nEpoch 3/10\n26/26 - 1s - 40ms/step - accuracy: 0.8861 - loss: 0.3671\nEpoch 4/10\n26/26 - 1s - 40ms/step - accuracy: 0.8156 - loss: 0.5120\nEpoch 5/10\n26/26 - 1s - 50ms/step - accuracy: 0.9220 - loss: 0.2281\nEpoch 6/10\n26/26 - 1s - 43ms/step - accuracy: 0.9616 - loss: 0.1413\nEpoch 7/10\n26/26 - 1s - 41ms/step - accuracy: 0.9765 - loss: 0.0984\nEpoch 8/10\n26/26 - 1s - 49ms/step - accuracy: 0.8403 - loss: 1.1478\nEpoch 9/10\n26/26 - 1s - 40ms/step - accuracy: 0.6993 - loss: 0.5251\nEpoch 10/10\n26/26 - 1s - 51ms/step - accuracy: 0.8725 - loss: 0.3351\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\nTamil - CNN - Precision: 0.3055555555555556, Recall: 0.22916666666666666, F1: 0.2619047619047619, Accuracy: 0.38\n\nEvaluating GRU model for Malayalam:\nEpoch 1/10\n25/25 - 7s - 267ms/step - accuracy: 0.5412 - loss: 0.6888\nEpoch 2/10\n25/25 - 5s - 197ms/step - accuracy: 0.8975 - loss: 0.5381\nEpoch 3/10\n25/25 - 6s - 229ms/step - accuracy: 0.9300 - loss: 0.1985\nEpoch 4/10\n25/25 - 7s - 268ms/step - accuracy: 0.9925 - loss: 0.0542\nEpoch 5/10\n25/25 - 6s - 249ms/step - accuracy: 1.0000 - loss: 0.0090\nEpoch 6/10\n25/25 - 5s - 203ms/step - accuracy: 1.0000 - loss: 0.0024\nEpoch 7/10\n25/25 - 5s - 189ms/step - accuracy: 1.0000 - loss: 9.3164e-04\nEpoch 8/10\n25/25 - 4s - 180ms/step - accuracy: 1.0000 - loss: 5.3645e-04\nEpoch 9/10\n25/25 - 4s - 175ms/step - accuracy: 1.0000 - loss: 3.7386e-04\nEpoch 10/10\n25/25 - 5s - 185ms/step - accuracy: 1.0000 - loss: 2.9182e-04\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step\nMalayalam - GRU - Precision: 0.5929203539823009, Recall: 0.67, F1: 0.6291079812206573, Accuracy: 0.605\n\nEvaluating GRU model for Tamil:\nEpoch 1/10\n26/26 - 7s - 286ms/step - accuracy: 0.7611 - loss: 0.5487\nEpoch 2/10\n26/26 - 5s - 178ms/step - accuracy: 0.9616 - loss: 0.1345\nEpoch 3/10\n26/26 - 5s - 200ms/step - accuracy: 0.9938 - loss: 0.0346\nEpoch 4/10\n26/26 - 5s - 194ms/step - accuracy: 0.9988 - loss: 0.0113\nEpoch 5/10\n26/26 - 6s - 230ms/step - accuracy: 1.0000 - loss: 0.0045\nEpoch 6/10\n26/26 - 6s - 229ms/step - accuracy: 1.0000 - loss: 0.0018\nEpoch 7/10\n26/26 - 5s - 191ms/step - accuracy: 1.0000 - loss: 9.3523e-04\nEpoch 8/10\n26/26 - 5s - 193ms/step - accuracy: 1.0000 - loss: 6.4615e-04\nEpoch 9/10\n26/26 - 5s - 197ms/step - accuracy: 1.0000 - loss: 5.0733e-04\nEpoch 10/10\n26/26 - 5s - 185ms/step - accuracy: 1.0000 - loss: 3.9815e-04\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 166ms/step\nTamil - GRU - Precision: 0.7, Recall: 0.4375, F1: 0.5384615384615384, Accuracy: 0.64\n\nEvaluating CNN-LSTM model for Malayalam:\nEpoch 1/10\n25/25 - 6s - 229ms/step - accuracy: 0.5975 - loss: 0.6830\nEpoch 2/10\n25/25 - 3s - 124ms/step - accuracy: 0.8037 - loss: 0.5326\nEpoch 3/10\n25/25 - 3s - 120ms/step - accuracy: 0.9175 - loss: 0.2317\nEpoch 4/10\n25/25 - 3s - 109ms/step - accuracy: 0.9812 - loss: 0.0669\nEpoch 5/10\n25/25 - 3s - 106ms/step - accuracy: 0.9962 - loss: 0.0165\nEpoch 6/10\n25/25 - 2s - 98ms/step - accuracy: 0.9987 - loss: 0.0065\nEpoch 7/10\n25/25 - 3s - 118ms/step - accuracy: 1.0000 - loss: 0.0041\nEpoch 8/10\n25/25 - 5s - 199ms/step - accuracy: 1.0000 - loss: 0.0026\nEpoch 9/10\n25/25 - 5s - 206ms/step - accuracy: 1.0000 - loss: 0.0017\nEpoch 10/10\n25/25 - 3s - 132ms/step - accuracy: 1.0000 - loss: 0.0012\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step\nMalayalam - CNN-LSTM - Precision: 0.8387096774193549, Recall: 0.26, F1: 0.39694656488549623, Accuracy: 0.605\n\nEvaluating CNN-LSTM model for Tamil:\nEpoch 1/10\n26/26 - 6s - 247ms/step - accuracy: 0.7735 - loss: 0.4984\nEpoch 2/10\n26/26 - 3s - 113ms/step - accuracy: 0.9394 - loss: 0.1724\nEpoch 3/10\n26/26 - 3s - 106ms/step - accuracy: 0.9913 - loss: 0.0467\nEpoch 4/10\n26/26 - 3s - 110ms/step - accuracy: 0.9988 - loss: 0.0136\nEpoch 5/10\n26/26 - 5s - 190ms/step - accuracy: 0.9975 - loss: 0.0081\nEpoch 6/10\n26/26 - 3s - 107ms/step - accuracy: 0.9988 - loss: 0.0063\nEpoch 7/10\n26/26 - 3s - 113ms/step - accuracy: 1.0000 - loss: 0.0028\nEpoch 8/10\n26/26 - 3s - 104ms/step - accuracy: 0.9988 - loss: 0.0022\nEpoch 9/10\n26/26 - 3s - 103ms/step - accuracy: 1.0000 - loss: 0.0010\nEpoch 10/10\n26/26 - 3s - 106ms/step - accuracy: 1.0000 - loss: 5.7367e-04\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step\nTamil - CNN-LSTM - Precision: 0.6666666666666666, Recall: 0.25, F1: 0.36363636363636365, Accuracy: 0.58\n\nEvaluating CNN-BiLSTM model for Malayalam:\nEpoch 1/10\n25/25 - 7s - 296ms/step - accuracy: 0.4950 - loss: 0.6932\nEpoch 2/10\n25/25 - 3s - 124ms/step - accuracy: 0.7425 - loss: 0.5952\nEpoch 3/10\n25/25 - 6s - 229ms/step - accuracy: 0.9663 - loss: 0.1272\nEpoch 4/10\n25/25 - 4s - 140ms/step - accuracy: 0.9975 - loss: 0.0106\nEpoch 5/10\n25/25 - 3s - 127ms/step - accuracy: 0.9987 - loss: 0.0031\nEpoch 6/10\n25/25 - 5s - 204ms/step - accuracy: 1.0000 - loss: 6.9918e-04\nEpoch 7/10\n25/25 - 5s - 217ms/step - accuracy: 1.0000 - loss: 4.4626e-04\nEpoch 8/10\n25/25 - 3s - 128ms/step - accuracy: 1.0000 - loss: 3.4410e-04\nEpoch 9/10\n25/25 - 3s - 121ms/step - accuracy: 1.0000 - loss: 2.7669e-04\nEpoch 10/10\n25/25 - 3s - 120ms/step - accuracy: 1.0000 - loss: 2.2803e-04\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step\nMalayalam - CNN-BiLSTM - Precision: 0.7868852459016393, Recall: 0.48, F1: 0.5962732919254659, Accuracy: 0.675\n\nEvaluating CNN-BiLSTM model for Tamil:\nEpoch 1/10\n26/26 - 8s - 294ms/step - accuracy: 0.7252 - loss: 0.7890\nEpoch 2/10\n26/26 - 3s - 129ms/step - accuracy: 0.9517 - loss: 0.1618\nEpoch 3/10\n26/26 - 4s - 140ms/step - accuracy: 0.9851 - loss: 0.0630\nEpoch 4/10\n26/26 - 4s - 143ms/step - accuracy: 0.9975 - loss: 0.0230\nEpoch 5/10\n26/26 - 5s - 210ms/step - accuracy: 0.9975 - loss: 0.0099\nEpoch 6/10\n26/26 - 4s - 151ms/step - accuracy: 1.0000 - loss: 0.0052\nEpoch 7/10\n26/26 - 3s - 131ms/step - accuracy: 1.0000 - loss: 0.0032\nEpoch 8/10\n26/26 - 5s - 196ms/step - accuracy: 1.0000 - loss: 0.0024\nEpoch 9/10\n26/26 - 3s - 131ms/step - accuracy: 1.0000 - loss: 0.0018\nEpoch 10/10\n26/26 - 4s - 135ms/step - accuracy: 1.0000 - loss: 0.0014\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 142ms/step\nTamil - CNN-BiLSTM - Precision: 0.7, Recall: 0.4375, F1: 0.5384615384615384, Accuracy: 0.64\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalAveragePooling1D, Dense, LSTM, Bidirectional, Attention, Input\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix\n",
        "\n",
        "# Parameters\n",
        "vocab_size = 10000\n",
        "max_length = 200\n",
        "embedding_dim = 100  # Embedding dimension\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "# Load datasets\n",
        "mal_train = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/mal_training_data_hum_ai.csv')\n",
        "mal_test = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/mal_test.xlsx - Sheet1.csv')\n",
        "tam_train = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/tam_training_data_hum_ai.csv')\n",
        "tam_test = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/tamil-test.xlsx - Sheet1.csv')\n",
        "\n",
        "# Normalize labels\n",
        "label_mapping = {'human': 0, 'ai': 1}\n",
        "def convert_labels(labels):\n",
        "    return np.array([label_mapping[label.lower()] for label in labels])\n",
        "\n",
        "mal_train_labels = convert_labels(mal_train['LABEL'])\n",
        "mal_test_labels = convert_labels(mal_test['Label'])\n",
        "tam_train_labels = convert_labels(tam_train['LABEL'])\n",
        "tam_test_labels = convert_labels(tam_test['Label'])\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(pd.concat([mal_train['DATA'], tam_train['DATA']]))  # Combine text for tokenizing\n",
        "mal_train_seq = pad_sequences(tokenizer.texts_to_sequences(mal_train['DATA']), maxlen=max_length)\n",
        "mal_test_seq = pad_sequences(tokenizer.texts_to_sequences(mal_test['DATA']), maxlen=max_length)\n",
        "tam_train_seq = pad_sequences(tokenizer.texts_to_sequences(tam_train['DATA']), maxlen=max_length)\n",
        "tam_test_seq = pad_sequences(tokenizer.texts_to_sequences(tam_test['Data']), maxlen=max_length)\n",
        "\n",
        "# Model definitions\n",
        "def create_cnn_model():\n",
        "    return Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        Conv1D(128, 5, activation='relu'),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "def create_bilstm_model():\n",
        "    return Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        Bidirectional(LSTM(64)),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "def create_bilstm_attention_model():\n",
        "    input_layer = Input(shape=(max_length,))\n",
        "    embedding_layer = Embedding(vocab_size, embedding_dim, input_length=max_length)(input_layer)\n",
        "    bilstm_layer = Bidirectional(LSTM(64, return_sequences=True))(embedding_layer)\n",
        "    attention_layer = Attention()([bilstm_layer, bilstm_layer])\n",
        "    attention_pooling = GlobalAveragePooling1D()(attention_layer)\n",
        "    dense_layer = Dense(24, activation='relu')(attention_pooling)\n",
        "    output_layer = Dense(1, activation='sigmoid')(dense_layer)\n",
        "    return tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Function to calculate G-mean\n",
        "def calculate_gmean(conf_matrix):\n",
        "    TN, FP, FN, TP = conf_matrix.ravel()\n",
        "    sensitivity = TP / (TP + FN)  # Recall\n",
        "    specificity = TN / (TN + FP)\n",
        "    gmean = np.sqrt(sensitivity * specificity)\n",
        "    return gmean\n",
        "\n",
        "# Train and evaluate function\n",
        "def train_and_evaluate(model, train_data, train_labels, test_data, test_labels, language, model_name):\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.fit(train_data, train_labels, epochs=10, verbose=2)\n",
        "    predictions = (model.predict(test_data) > 0.5).astype(int)\n",
        "\n",
        "    # Compute metrics\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(test_labels, predictions, average='binary')\n",
        "    accuracy = accuracy_score(test_labels, predictions)\n",
        "    conf_matrix = confusion_matrix(test_labels, predictions)\n",
        "    gmean = calculate_gmean(conf_matrix)\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"{language} - {model_name}:\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1: {f1}\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"G-mean: {gmean}\")\n",
        "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "# Instantiate and evaluate models\n",
        "models = {\n",
        "    'CNN': create_cnn_model(),\n",
        "    'BiLSTM': create_bilstm_model(),\n",
        "    'BiLSTM + Attention': create_bilstm_attention_model()\n",
        "}\n",
        "\n",
        "# Evaluate for Malayalam and Tamil\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nEvaluating {model_name} model for Malayalam:\")\n",
        "    train_and_evaluate(model, mal_train_seq, mal_train_labels, mal_test_seq, mal_test_labels, \"Malayalam\", model_name)\n",
        "    print(f\"\\nEvaluating {model_name} model for Tamil:\")\n",
        "    train_and_evaluate(model, tam_train_seq, tam_train_labels, tam_test_seq, tam_test_labels, \"Tamil\", model_name)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T20:13:23.243388Z",
          "iopub.execute_input": "2025-01-30T20:13:23.243818Z",
          "iopub.status.idle": "2025-01-30T20:16:49.638798Z",
          "shell.execute_reply.started": "2025-01-30T20:13:23.243787Z",
          "shell.execute_reply": "2025-01-30T20:16:49.637608Z"
        },
        "id": "M7X0snECJaUw",
        "outputId": "707a1938-53e4-44f9-ae6b-376fd34e63e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nEvaluating CNN model for Malayalam:\nEpoch 1/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "25/25 - 3s - 105ms/step - accuracy: 0.4825 - loss: 0.6938\nEpoch 2/10\n25/25 - 1s - 33ms/step - accuracy: 0.5263 - loss: 0.6919\nEpoch 3/10\n25/25 - 1s - 34ms/step - accuracy: 0.6587 - loss: 0.6666\nEpoch 4/10\n25/25 - 1s - 52ms/step - accuracy: 0.8475 - loss: 0.5334\nEpoch 5/10\n25/25 - 1s - 39ms/step - accuracy: 0.8700 - loss: 0.3706\nEpoch 6/10\n25/25 - 1s - 39ms/step - accuracy: 0.9013 - loss: 0.2963\nEpoch 7/10\n25/25 - 1s - 38ms/step - accuracy: 0.9488 - loss: 0.1606\nEpoch 8/10\n25/25 - 1s - 36ms/step - accuracy: 0.9737 - loss: 0.1056\nEpoch 9/10\n25/25 - 1s - 37ms/step - accuracy: 0.9850 - loss: 0.0698\nEpoch 10/10\n25/25 - 1s - 40ms/step - accuracy: 0.9912 - loss: 0.0426\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\nMalayalam - CNN:\nPrecision: 0.7181818181818181\nRecall: 0.79\nF1: 0.7523809523809524\nAccuracy: 0.74\nG-mean: 0.7383088784512889\nConfusion Matrix:\n[[69 31]\n [21 79]]\n==================================================\n\nEvaluating CNN model for Tamil:\nEpoch 1/10\n26/26 - 3s - 96ms/step - accuracy: 0.5198 - loss: 1.0253\nEpoch 2/10\n26/26 - 1s - 52ms/step - accuracy: 0.6733 - loss: 0.5950\nEpoch 3/10\n26/26 - 1s - 37ms/step - accuracy: 0.7983 - loss: 0.4979\nEpoch 4/10\n26/26 - 1s - 34ms/step - accuracy: 0.8490 - loss: 0.4176\nEpoch 5/10\n26/26 - 1s - 34ms/step - accuracy: 0.8837 - loss: 0.3389\nEpoch 6/10\n26/26 - 1s - 35ms/step - accuracy: 0.9220 - loss: 0.2403\nEpoch 7/10\n26/26 - 1s - 34ms/step - accuracy: 0.9356 - loss: 0.1730\nEpoch 8/10\n26/26 - 1s - 35ms/step - accuracy: 0.9752 - loss: 0.1157\nEpoch 9/10\n26/26 - 1s - 41ms/step - accuracy: 0.9765 - loss: 0.0889\nEpoch 10/10\n26/26 - 1s - 41ms/step - accuracy: 0.9889 - loss: 0.0544\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\nTamil - CNN:\nPrecision: 0.6206896551724138\nRecall: 0.375\nF1: 0.4675324675324676\nAccuracy: 0.59\nG-mean: 0.5437582890614882\nConfusion Matrix:\n[[41 11]\n [30 18]]\n==================================================\n\nEvaluating BiLSTM model for Malayalam:\nEpoch 1/10\n25/25 - 8s - 309ms/step - accuracy: 0.5725 - loss: 0.6844\nEpoch 2/10\n25/25 - 3s - 116ms/step - accuracy: 0.7750 - loss: 0.5326\nEpoch 3/10\n25/25 - 3s - 124ms/step - accuracy: 0.9463 - loss: 0.2246\nEpoch 4/10\n25/25 - 3s - 120ms/step - accuracy: 0.9925 - loss: 0.0593\nEpoch 5/10\n25/25 - 3s - 121ms/step - accuracy: 1.0000 - loss: 0.0132\nEpoch 6/10\n25/25 - 3s - 116ms/step - accuracy: 1.0000 - loss: 0.0046\nEpoch 7/10\n25/25 - 3s - 123ms/step - accuracy: 1.0000 - loss: 0.0026\nEpoch 8/10\n25/25 - 5s - 211ms/step - accuracy: 1.0000 - loss: 0.0018\nEpoch 9/10\n25/25 - 4s - 143ms/step - accuracy: 1.0000 - loss: 0.0013\nEpoch 10/10\n25/25 - 4s - 178ms/step - accuracy: 1.0000 - loss: 0.0010\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step\nMalayalam - BiLSTM:\nPrecision: 0.7319587628865979\nRecall: 0.71\nF1: 0.7208121827411167\nAccuracy: 0.725\nG-mean: 0.7248448109768049\nConfusion Matrix:\n[[74 26]\n [29 71]]\n==================================================\n\nEvaluating BiLSTM model for Tamil:\nEpoch 1/10\n26/26 - 7s - 273ms/step - accuracy: 0.7871 - loss: 0.4928\nEpoch 2/10\n26/26 - 3s - 123ms/step - accuracy: 0.9666 - loss: 0.1395\nEpoch 3/10\n26/26 - 3s - 125ms/step - accuracy: 0.9913 - loss: 0.0585\nEpoch 4/10\n26/26 - 3s - 126ms/step - accuracy: 0.9975 - loss: 0.0312\nEpoch 5/10\n26/26 - 3s - 125ms/step - accuracy: 0.9988 - loss: 0.0181\nEpoch 6/10\n26/26 - 4s - 136ms/step - accuracy: 0.9988 - loss: 0.0114\nEpoch 7/10\n26/26 - 4s - 142ms/step - accuracy: 1.0000 - loss: 0.0070\nEpoch 8/10\n26/26 - 3s - 127ms/step - accuracy: 1.0000 - loss: 0.0049\nEpoch 9/10\n26/26 - 4s - 153ms/step - accuracy: 1.0000 - loss: 0.0040\nEpoch 10/10\n26/26 - 4s - 157ms/step - accuracy: 1.0000 - loss: 0.0031\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 145ms/step\nTamil - BiLSTM:\nPrecision: 0.75\nRecall: 0.5\nF1: 0.6\nAccuracy: 0.68\nG-mean: 0.6504436355879909\nConfusion Matrix:\n[[44  8]\n [24 24]]\n==================================================\n\nEvaluating BiLSTM + Attention model for Malayalam:\nEpoch 1/10\n25/25 - 9s - 366ms/step - accuracy: 0.4950 - loss: 0.6954\nEpoch 2/10\n25/25 - 5s - 199ms/step - accuracy: 0.4863 - loss: 0.6946\nEpoch 3/10\n25/25 - 5s - 203ms/step - accuracy: 0.4850 - loss: 0.6935\nEpoch 4/10\n25/25 - 4s - 165ms/step - accuracy: 0.4725 - loss: 0.6934\nEpoch 5/10\n25/25 - 4s - 170ms/step - accuracy: 0.4875 - loss: 0.6932\nEpoch 6/10\n25/25 - 4s - 162ms/step - accuracy: 0.4913 - loss: 0.6932\nEpoch 7/10\n25/25 - 4s - 162ms/step - accuracy: 0.5175 - loss: 0.6931\nEpoch 8/10\n25/25 - 4s - 157ms/step - accuracy: 0.5075 - loss: 0.6929\nEpoch 9/10\n25/25 - 4s - 159ms/step - accuracy: 0.5013 - loss: 0.6929\nEpoch 10/10\n25/25 - 4s - 162ms/step - accuracy: 0.5088 - loss: 0.6927\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step\nMalayalam - BiLSTM + Attention:\nPrecision: 0.5102040816326531\nRecall: 1.0\nF1: 0.6756756756756758\nAccuracy: 0.52\nG-mean: 0.2\nConfusion Matrix:\n[[  4  96]\n [  0 100]]\n==================================================\n\nEvaluating BiLSTM + Attention model for Tamil:\nEpoch 1/10\n26/26 - 10s - 389ms/step - accuracy: 0.5087 - loss: 0.6935\nEpoch 2/10\n26/26 - 5s - 185ms/step - accuracy: 0.5012 - loss: 0.6933\nEpoch 3/10\n26/26 - 5s - 188ms/step - accuracy: 0.4988 - loss: 0.6932\nEpoch 4/10\n26/26 - 5s - 183ms/step - accuracy: 0.4988 - loss: 0.6932\nEpoch 5/10\n26/26 - 5s - 180ms/step - accuracy: 0.4988 - loss: 0.6932\nEpoch 6/10\n26/26 - 5s - 201ms/step - accuracy: 0.4988 - loss: 0.6932\nEpoch 7/10\n26/26 - 5s - 178ms/step - accuracy: 0.4988 - loss: 0.6932\nEpoch 8/10\n26/26 - 4s - 168ms/step - accuracy: 0.4988 - loss: 0.6932\nEpoch 9/10\n26/26 - 5s - 180ms/step - accuracy: 0.4988 - loss: 0.6932\nEpoch 10/10\n26/26 - 4s - 168ms/step - accuracy: 0.4988 - loss: 0.6932\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step\nTamil - BiLSTM + Attention:\nPrecision: 0.0\nRecall: 0.0\nF1: 0.0\nAccuracy: 0.52\nG-mean: 0.0\nConfusion Matrix:\n[[52  0]\n [48  0]]\n==================================================\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Satisfy all conditions for DL models for mala tam"
      ],
      "metadata": {
        "id": "Qa3WdZ_sJaUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalAveragePooling1D, Dense, LSTM, Bidirectional, Attention, Input\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix\n",
        "\n",
        "# Parameters\n",
        "vocab_size = 10000\n",
        "max_length = 200\n",
        "embedding_dim = 100  # Embedding dimension\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "# Load datasets\n",
        "mal_train = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/mal_training_data_hum_ai.csv')\n",
        "mal_test = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/mal_test.xlsx - Sheet1.csv')\n",
        "tam_train = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/tam_training_data_hum_ai.csv')\n",
        "tam_test = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/tamil-test.xlsx - Sheet1.csv')\n",
        "\n",
        "# Normalize labels\n",
        "label_mapping = {'human': 0, 'ai': 1}\n",
        "def convert_labels(labels):\n",
        "    return np.array([label_mapping[label.lower()] for label in labels])\n",
        "\n",
        "mal_train_labels = convert_labels(mal_train['LABEL'])\n",
        "mal_test_labels = convert_labels(mal_test['Label'])\n",
        "tam_train_labels = convert_labels(tam_train['LABEL'])\n",
        "tam_test_labels = convert_labels(tam_test['Label'])\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(pd.concat([mal_train['DATA'], tam_train['DATA']]))  # Combine text for tokenizing\n",
        "mal_train_seq = pad_sequences(tokenizer.texts_to_sequences(mal_train['DATA']), maxlen=max_length)\n",
        "mal_test_seq = pad_sequences(tokenizer.texts_to_sequences(mal_test['DATA']), maxlen=max_length)\n",
        "tam_train_seq = pad_sequences(tokenizer.texts_to_sequences(tam_train['DATA']), maxlen=max_length)\n",
        "tam_test_seq = pad_sequences(tokenizer.texts_to_sequences(tam_test['Data']), maxlen=max_length)\n",
        "\n",
        "# Model definitions\n",
        "def create_cnn_model():\n",
        "    return Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        Conv1D(128, 5, activation='relu'),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        " F\n",
        "def create_bilstm_model():\n",
        "    return Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        Bidirectional(LSTM(64)),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "def create_bilstm_attention_model():\n",
        "    input_layer = Input(shape=(max_length,))\n",
        "    embedding_layer = Embedding(vocab_size, embedding_dim, input_length=max_length)(input_layer)\n",
        "    bilstm_layer = Bidirectional(LSTM(64, return_sequences=True))(embedding_layer)\n",
        "    attention_layer = Attention()([bilstm_layer, bilstm_layer])\n",
        "    attention_pooling = GlobalAveragePooling1D()(attention_layer)\n",
        "    dense_layer = Dense(24, activation='relu')(attention_pooling)\n",
        "    output_layer = Dense(1, activation='sigmoid')(dense_layer)\n",
        "    return tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "def create_keras_cnn_model():\n",
        "    return Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        Conv1D(128, 5, activation='relu'),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "def create_glove_cnn_model():\n",
        "    return Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length, trainable=False),\n",
        "        Conv1D(128, 5, activation='relu'),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "def create_glove_bilstm_model():\n",
        "    return Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length, trainable=False),\n",
        "        Bidirectional(LSTM(64)),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "# Function to calculate G-mean\n",
        "def calculate_gmean(conf_matrix):\n",
        "    TN, FP, FN, TP = conf_matrix.ravel()\n",
        "    sensitivity = TP / (TP + FN)  # Recall\n",
        "    specificity = TN / (TN + FP)\n",
        "    gmean = np.sqrt(sensitivity * specificity)\n",
        "    return gmean\n",
        "\n",
        "# Train and evaluate function\n",
        "def train_and_evaluate(model, train_data, train_labels, test_data, test_labels, language, model_name):\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.fit(train_data, train_labels, epochs=10, verbose=2)\n",
        "    predictions = (model.predict(test_data) > 0.5).astype(int)\n",
        "\n",
        "    # Compute metrics\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(test_labels, predictions, average='binary')\n",
        "    accuracy = accuracy_score(test_labels, predictions)\n",
        "    conf_matrix = confusion_matrix(test_labels, predictions)\n",
        "    gmean = calculate_gmean(conf_matrix)\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"{language} - {model_name}:\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1: {f1}\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"G-mean: {gmean}\")\n",
        "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "# Instantiate and evaluate models\n",
        "models = {\n",
        "    'CNN': create_cnn_model(),\n",
        "    'BiLSTM': create_bilstm_model(),\n",
        "    'BiLSTM + Attention': create_bilstm_attention_model(),\n",
        "    'Keras + CNN': create_keras_cnn_model(),\n",
        "    'GloVe + CNN': create_glove_cnn_model(),\n",
        "    'GloVe + BiLSTM': create_glove_bilstm_model()\n",
        "}\n",
        "\n",
        "# Evaluate for Malayalam and Tamil\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nEvaluating {model_name} model for Malayalam:\")\n",
        "    train_and_evaluate(model, mal_train_seq, mal_train_labels, mal_test_seq, mal_test_labels, \"Malayalam\", model_name)\n",
        "    print(f\"\\nEvaluating {model_name} model for Tamil:\")\n",
        "    train_and_evaluate(model, tam_train_seq, tam_train_labels, tam_test_seq, tam_test_labels, \"Tamil\", model_name)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T20:24:03.73623Z",
          "iopub.execute_input": "2025-01-30T20:24:03.736641Z",
          "iopub.status.idle": "2025-01-30T20:29:18.217841Z",
          "shell.execute_reply.started": "2025-01-30T20:24:03.736608Z",
          "shell.execute_reply": "2025-01-30T20:29:18.216427Z"
        },
        "id": "F9FvSW7yJaUx",
        "outputId": "1ab3fc81-6ea7-40d7-dfa7-474010c16ff1"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEvaluating CNN model for Malayalam:\nEpoch 1/10\n25/25 - 3s - 134ms/step - accuracy: 0.4812 - loss: 0.6938\nEpoch 2/10\n25/25 - 1s - 39ms/step - accuracy: 0.5038 - loss: 0.6972\nEpoch 3/10\n25/25 - 1s - 53ms/step - accuracy: 0.5938 - loss: 0.6879\nEpoch 4/10\n25/25 - 1s - 45ms/step - accuracy: 0.7425 - loss: 0.6459\nEpoch 5/10\n25/25 - 1s - 43ms/step - accuracy: 0.8200 - loss: 0.4975\nEpoch 6/10\n25/25 - 1s - 39ms/step - accuracy: 0.9038 - loss: 0.3049\nEpoch 7/10\n25/25 - 1s - 55ms/step - accuracy: 0.9475 - loss: 0.1867\nEpoch 8/10\n25/25 - 1s - 53ms/step - accuracy: 0.9613 - loss: 0.1358\nEpoch 9/10\n25/25 - 1s - 36ms/step - accuracy: 0.9812 - loss: 0.0822\nEpoch 10/10\n25/25 - 1s - 60ms/step - accuracy: 0.9887 - loss: 0.0576\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\nMalayalam - CNN:\nPrecision: 0.7476635514018691\nRecall: 0.8\nF1: 0.7729468599033816\nAccuracy: 0.765\nG-mean: 0.764198926981712\nConfusion Matrix:\n[[73 27]\n [20 80]]\n==================================================\n\nEvaluating CNN model for Tamil:\nEpoch 1/10\n26/26 - 3s - 119ms/step - accuracy: 0.5631 - loss: 1.0337\nEpoch 2/10\n26/26 - 1s - 46ms/step - accuracy: 0.7599 - loss: 0.5731\nEpoch 3/10\n26/26 - 1s - 47ms/step - accuracy: 0.8119 - loss: 0.4957\nEpoch 4/10\n26/26 - 1s - 54ms/step - accuracy: 0.8663 - loss: 0.4075\nEpoch 5/10\n26/26 - 1s - 41ms/step - accuracy: 0.9134 - loss: 0.3088\nEpoch 6/10\n26/26 - 1s - 38ms/step - accuracy: 0.9332 - loss: 0.2132\nEpoch 7/10\n26/26 - 1s - 49ms/step - accuracy: 0.9554 - loss: 0.1457\nEpoch 8/10\n26/26 - 1s - 38ms/step - accuracy: 0.9777 - loss: 0.0969\nEpoch 9/10\n26/26 - 1s - 47ms/step - accuracy: 0.9814 - loss: 0.0710\nEpoch 10/10\n26/26 - 1s - 37ms/step - accuracy: 0.9864 - loss: 0.0571\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\nTamil - CNN:\nPrecision: 0.6363636363636364\nRecall: 0.2916666666666667\nF1: 0.4\nAccuracy: 0.58\nG-mean: 0.4967845325640401\nConfusion Matrix:\n[[44  8]\n [34 14]]\n==================================================\n\nEvaluating BiLSTM model for Malayalam:\nEpoch 1/10\n25/25 - 7s - 293ms/step - accuracy: 0.5650 - loss: 0.6835\nEpoch 2/10\n25/25 - 3s - 115ms/step - accuracy: 0.7600 - loss: 0.5581\nEpoch 3/10\n25/25 - 3s - 118ms/step - accuracy: 0.9300 - loss: 0.2404\nEpoch 4/10\n25/25 - 3s - 109ms/step - accuracy: 0.9875 - loss: 0.0665\nEpoch 5/10\n25/25 - 3s - 112ms/step - accuracy: 1.0000 - loss: 0.0173\nEpoch 6/10\n25/25 - 3s - 111ms/step - accuracy: 1.0000 - loss: 0.0074\nEpoch 7/10\n25/25 - 4s - 141ms/step - accuracy: 1.0000 - loss: 0.0031\nEpoch 8/10\n25/25 - 5s - 187ms/step - accuracy: 1.0000 - loss: 0.0019\nEpoch 9/10\n25/25 - 5s - 207ms/step - accuracy: 1.0000 - loss: 0.0017\nEpoch 10/10\n25/25 - 5s - 202ms/step - accuracy: 1.0000 - loss: 0.0011\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step\nMalayalam - BiLSTM:\nPrecision: 0.8536585365853658\nRecall: 0.35\nF1: 0.49645390070921985\nAccuracy: 0.645\nG-mean: 0.5735852159879994\nConfusion Matrix:\n[[94  6]\n [65 35]]\n==================================================\n\nEvaluating BiLSTM model for Tamil:\nEpoch 1/10\n26/26 - 7s - 279ms/step - accuracy: 0.7500 - loss: 0.5381\nEpoch 2/10\n26/26 - 3s - 129ms/step - accuracy: 0.9567 - loss: 0.1836\nEpoch 3/10\n26/26 - 4s - 157ms/step - accuracy: 0.9876 - loss: 0.0876\nEpoch 4/10\n26/26 - 4s - 165ms/step - accuracy: 0.9963 - loss: 0.0518\nEpoch 5/10\n26/26 - 3s - 123ms/step - accuracy: 0.9963 - loss: 0.0309\nEpoch 6/10\n26/26 - 3s - 125ms/step - accuracy: 0.9975 - loss: 0.0234\nEpoch 7/10\n26/26 - 3s - 124ms/step - accuracy: 0.9988 - loss: 0.0144\nEpoch 8/10\n26/26 - 3s - 127ms/step - accuracy: 1.0000 - loss: 0.0108\nEpoch 9/10\n26/26 - 3s - 125ms/step - accuracy: 1.0000 - loss: 0.0075\nEpoch 10/10\n26/26 - 3s - 122ms/step - accuracy: 1.0000 - loss: 0.0060\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step\nTamil - BiLSTM:\nPrecision: 0.7307692307692307\nRecall: 0.3958333333333333\nF1: 0.5135135135135135\nAccuracy: 0.64\nG-mean: 0.5852760689820462\nConfusion Matrix:\n[[45  7]\n [29 19]]\n==================================================\n\nEvaluating BiLSTM + Attention model for Malayalam:\nEpoch 1/10\n25/25 - 9s - 342ms/step - accuracy: 0.4825 - loss: 0.6954\nEpoch 2/10\n25/25 - 5s - 200ms/step - accuracy: 0.5000 - loss: 0.6934\nEpoch 3/10\n25/25 - 4s - 157ms/step - accuracy: 0.4750 - loss: 0.6936\nEpoch 4/10\n25/25 - 4s - 158ms/step - accuracy: 0.5125 - loss: 0.6936\nEpoch 5/10\n25/25 - 4s - 163ms/step - accuracy: 0.5238 - loss: 0.6930\nEpoch 6/10\n25/25 - 4s - 160ms/step - accuracy: 0.5000 - loss: 0.6933\nEpoch 7/10\n25/25 - 4s - 154ms/step - accuracy: 0.4750 - loss: 0.6929\nEpoch 8/10\n25/25 - 6s - 228ms/step - accuracy: 0.5400 - loss: 0.6927\nEpoch 9/10\n25/25 - 4s - 152ms/step - accuracy: 0.5275 - loss: 0.6945\nEpoch 10/10\n25/25 - 5s - 204ms/step - accuracy: 0.5188 - loss: 0.6934\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step\nMalayalam - BiLSTM + Attention:\nPrecision: 0.5025125628140703\nRecall: 1.0\nF1: 0.6688963210702341\nAccuracy: 0.505\nG-mean: 0.1\nConfusion Matrix:\n[[  1  99]\n [  0 100]]\n==================================================\n\nEvaluating BiLSTM + Attention model for Tamil:\nEpoch 1/10\n26/26 - 8s - 321ms/step - accuracy: 0.4950 - loss: 0.6969\nEpoch 2/10\n26/26 - 4s - 172ms/step - accuracy: 0.5012 - loss: 0.6942\nEpoch 3/10\n26/26 - 5s - 179ms/step - accuracy: 0.4988 - loss: 0.6932\nEpoch 4/10\n26/26 - 5s - 193ms/step - accuracy: 0.4988 - loss: 0.6932\nEpoch 5/10\n26/26 - 4s - 173ms/step - accuracy: 0.4988 - loss: 0.6932\nEpoch 6/10\n26/26 - 4s - 171ms/step - accuracy: 0.4988 - loss: 0.6932\nEpoch 7/10\n26/26 - 5s - 199ms/step - accuracy: 0.4988 - loss: 0.6932\nEpoch 8/10\n26/26 - 5s - 193ms/step - accuracy: 0.4988 - loss: 0.6932\nEpoch 9/10\n26/26 - 4s - 172ms/step - accuracy: 0.4988 - loss: 0.6932\nEpoch 10/10\n26/26 - 4s - 172ms/step - accuracy: 0.4988 - loss: 0.6932\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step\nTamil - BiLSTM + Attention:\nPrecision: 0.0\nRecall: 0.0\nF1: 0.0\nAccuracy: 0.52\nG-mean: 0.0\nConfusion Matrix:\n[[52  0]\n [48  0]]\n==================================================\n\nEvaluating Keras + CNN model for Malayalam:\nEpoch 1/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "25/25 - 2s - 94ms/step - accuracy: 0.5150 - loss: 0.6936\nEpoch 2/10\n25/25 - 1s - 35ms/step - accuracy: 0.5713 - loss: 0.6900\nEpoch 3/10\n25/25 - 1s - 33ms/step - accuracy: 0.7563 - loss: 0.6559\nEpoch 4/10\n25/25 - 1s - 51ms/step - accuracy: 0.8075 - loss: 0.5239\nEpoch 5/10\n25/25 - 1s - 32ms/step - accuracy: 0.8950 - loss: 0.3503\nEpoch 6/10\n25/25 - 1s - 32ms/step - accuracy: 0.9212 - loss: 0.2288\nEpoch 7/10\n25/25 - 1s - 33ms/step - accuracy: 0.9600 - loss: 0.1335\nEpoch 8/10\n25/25 - 1s - 32ms/step - accuracy: 0.9787 - loss: 0.0784\nEpoch 9/10\n25/25 - 1s - 33ms/step - accuracy: 0.9925 - loss: 0.0491\nEpoch 10/10\n25/25 - 1s - 32ms/step - accuracy: 0.9912 - loss: 0.0362\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\nMalayalam - Keras + CNN:\nPrecision: 0.6984126984126984\nRecall: 0.88\nF1: 0.7787610619469026\nAccuracy: 0.75\nG-mean: 0.7386474125047755\nConfusion Matrix:\n[[62 38]\n [12 88]]\n==================================================\n\nEvaluating Keras + CNN model for Tamil:\nEpoch 1/10\n26/26 - 2s - 88ms/step - accuracy: 0.5371 - loss: 0.9506\nEpoch 2/10\n26/26 - 1s - 31ms/step - accuracy: 0.7290 - loss: 0.5650\nEpoch 3/10\n26/26 - 1s - 32ms/step - accuracy: 0.8317 - loss: 0.4685\nEpoch 4/10\n26/26 - 1s - 31ms/step - accuracy: 0.8973 - loss: 0.3722\nEpoch 5/10\n26/26 - 1s - 32ms/step - accuracy: 0.9158 - loss: 0.2701\nEpoch 6/10\n26/26 - 1s - 34ms/step - accuracy: 0.9431 - loss: 0.1740\nEpoch 7/10\n26/26 - 1s - 32ms/step - accuracy: 0.9715 - loss: 0.1167\nEpoch 8/10\n26/26 - 1s - 32ms/step - accuracy: 0.9790 - loss: 0.0807\nEpoch 9/10\n26/26 - 1s - 32ms/step - accuracy: 0.9864 - loss: 0.0578\nEpoch 10/10\n26/26 - 1s - 49ms/step - accuracy: 0.9901 - loss: 0.0439\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\nTamil - Keras + CNN:\nPrecision: 0.5714285714285714\nRecall: 0.4166666666666667\nF1: 0.48192771084337344\nAccuracy: 0.57\nG-mean: 0.5444945903995364\nConfusion Matrix:\n[[37 15]\n [28 20]]\n==================================================\n\nEvaluating GloVe + CNN model for Malayalam:\nEpoch 1/10\n25/25 - 2s - 84ms/step - accuracy: 0.4950 - loss: 0.6940\nEpoch 2/10\n25/25 - 1s - 23ms/step - accuracy: 0.4925 - loss: 0.6931\nEpoch 3/10\n25/25 - 1s - 25ms/step - accuracy: 0.5000 - loss: 0.6931\nEpoch 4/10\n25/25 - 1s - 23ms/step - accuracy: 0.5125 - loss: 0.6927\nEpoch 5/10\n25/25 - 1s - 23ms/step - accuracy: 0.5125 - loss: 0.6925\nEpoch 6/10\n25/25 - 1s - 23ms/step - accuracy: 0.5462 - loss: 0.6905\nEpoch 7/10\n25/25 - 1s - 24ms/step - accuracy: 0.6338 - loss: 0.6886\nEpoch 8/10\n25/25 - 1s - 26ms/step - accuracy: 0.6237 - loss: 0.6855\nEpoch 9/10\n25/25 - 1s - 26ms/step - accuracy: 0.6438 - loss: 0.6809\nEpoch 10/10\n25/25 - 1s - 23ms/step - accuracy: 0.5838 - loss: 0.6764\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\nMalayalam - GloVe + CNN:\nPrecision: 0.0\nRecall: 0.0\nF1: 0.0\nAccuracy: 0.5\nG-mean: 0.0\nConfusion Matrix:\n[[100   0]\n [100   0]]\n==================================================\n\nEvaluating GloVe + CNN model for Tamil:\nEpoch 1/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "26/26 - 3s - 105ms/step - accuracy: 0.4728 - loss: 0.6978\nEpoch 2/10\n26/26 - 1s - 33ms/step - accuracy: 0.4530 - loss: 0.6990\nEpoch 3/10\n26/26 - 1s - 23ms/step - accuracy: 0.5074 - loss: 0.6947\nEpoch 4/10\n26/26 - 1s - 23ms/step - accuracy: 0.4889 - loss: 0.6968\nEpoch 5/10\n26/26 - 1s - 22ms/step - accuracy: 0.4765 - loss: 0.6942\nEpoch 6/10\n26/26 - 1s - 22ms/step - accuracy: 0.5012 - loss: 0.6919\nEpoch 7/10\n26/26 - 1s - 24ms/step - accuracy: 0.5470 - loss: 0.6900\nEpoch 8/10\n26/26 - 1s - 22ms/step - accuracy: 0.5309 - loss: 0.6932\nEpoch 9/10\n26/26 - 1s - 23ms/step - accuracy: 0.5136 - loss: 0.6912\nEpoch 10/10\n26/26 - 1s - 23ms/step - accuracy: 0.4988 - loss: 0.6948\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\nTamil - GloVe + CNN:\nPrecision: 0.0\nRecall: 0.0\nF1: 0.0\nAccuracy: 0.52\nG-mean: 0.0\nConfusion Matrix:\n[[52  0]\n [48  0]]\n==================================================\n\nEvaluating GloVe + BiLSTM model for Malayalam:\nEpoch 1/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "25/25 - 6s - 229ms/step - accuracy: 0.5587 - loss: 0.6855\nEpoch 2/10\n25/25 - 2s - 99ms/step - accuracy: 0.6162 - loss: 0.6518\nEpoch 3/10\n25/25 - 2s - 93ms/step - accuracy: 0.6662 - loss: 0.6096\nEpoch 4/10\n25/25 - 2s - 99ms/step - accuracy: 0.6737 - loss: 0.5768\nEpoch 5/10\n25/25 - 2s - 94ms/step - accuracy: 0.6825 - loss: 0.5656\nEpoch 6/10\n25/25 - 2s - 93ms/step - accuracy: 0.6963 - loss: 0.5511\nEpoch 7/10\n25/25 - 2s - 91ms/step - accuracy: 0.7088 - loss: 0.5339\nEpoch 8/10\n25/25 - 2s - 89ms/step - accuracy: 0.7300 - loss: 0.5116\nEpoch 9/10\n25/25 - 2s - 93ms/step - accuracy: 0.7387 - loss: 0.5040\nEpoch 10/10\n25/25 - 3s - 106ms/step - accuracy: 0.7525 - loss: 0.5069\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step\nMalayalam - GloVe + BiLSTM:\nPrecision: 0.7094017094017094\nRecall: 0.83\nF1: 0.7649769585253456\nAccuracy: 0.745\nG-mean: 0.7401351227985333\nConfusion Matrix:\n[[66 34]\n [17 83]]\n==================================================\n\nEvaluating GloVe + BiLSTM model for Tamil:\nEpoch 1/10\n26/26 - 6s - 230ms/step - accuracy: 0.6163 - loss: 0.6523\nEpoch 2/10\n26/26 - 3s - 101ms/step - accuracy: 0.6696 - loss: 0.6139\nEpoch 3/10\n26/26 - 3s - 100ms/step - accuracy: 0.6955 - loss: 0.5742\nEpoch 4/10\n26/26 - 5s - 202ms/step - accuracy: 0.7104 - loss: 0.5534\nEpoch 5/10\n26/26 - 3s - 110ms/step - accuracy: 0.7512 - loss: 0.5109\nEpoch 6/10\n26/26 - 3s - 112ms/step - accuracy: 0.7661 - loss: 0.4833\nEpoch 7/10\n26/26 - 3s - 104ms/step - accuracy: 0.7884 - loss: 0.4572\nEpoch 8/10\n26/26 - 3s - 106ms/step - accuracy: 0.7933 - loss: 0.4370\nEpoch 9/10\n26/26 - 3s - 114ms/step - accuracy: 0.8020 - loss: 0.4126\nEpoch 10/10\n26/26 - 3s - 113ms/step - accuracy: 0.8168 - loss: 0.3853\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step\nTamil - GloVe + BiLSTM:\nPrecision: 0.8235294117647058\nRecall: 0.5833333333333334\nF1: 0.6829268292682927\nAccuracy: 0.74\nG-mean: 0.7183492472506957\nConfusion Matrix:\n[[46  6]\n [20 28]]\n==================================================\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DL with macro F1-score"
      ],
      "metadata": {
        "id": "JeAKzrC3JaUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalAveragePooling1D, Dense, LSTM, Bidirectional, Attention, Input\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, f1_score\n",
        "\n",
        "# Parameters\n",
        "vocab_size = 10000\n",
        "max_length = 200\n",
        "embedding_dim = 100  # Embedding dimension\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "# Load datasets\n",
        "mal_train = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/mal_training_data_hum_ai.csv')\n",
        "mal_test = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/mal_test.xlsx - Sheet1.csv')\n",
        "tam_train = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/tam_training_data_hum_ai.csv')\n",
        "tam_test = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/tamil-test.xlsx - Sheet1.csv')\n",
        "\n",
        "# Normalize labels\n",
        "label_mapping = {'human': 0, 'ai': 1}\n",
        "def convert_labels(labels):\n",
        "    return np.array([label_mapping[label.lower()] for label in labels])\n",
        "\n",
        "mal_train_labels = convert_labels(mal_train['LABEL'])\n",
        "mal_test_labels = convert_labels(mal_test['Label'])\n",
        "tam_train_labels = convert_labels(tam_train['LABEL'])\n",
        "tam_test_labels = convert_labels(tam_test['Label'])\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(pd.concat([mal_train['DATA'], tam_train['DATA']]))  # Combine text for tokenizing\n",
        "mal_train_seq = pad_sequences(tokenizer.texts_to_sequences(mal_train['DATA']), maxlen=max_length)\n",
        "mal_test_seq = pad_sequences(tokenizer.texts_to_sequences(mal_test['DATA']), maxlen=max_length)\n",
        "tam_train_seq = pad_sequences(tokenizer.texts_to_sequences(tam_train['DATA']), maxlen=max_length)\n",
        "tam_test_seq = pad_sequences(tokenizer.texts_to_sequences(tam_test['Data']), maxlen=max_length)\n",
        "\n",
        "# Model definitions\n",
        "def create_cnn_model():\n",
        "    return Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        Conv1D(128, 5, activation='relu'),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "def create_bilstm_model():\n",
        "    return Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        Bidirectional(LSTM(64)),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "def create_bilstm_attention_model():\n",
        "    input_layer = Input(shape=(max_length,))\n",
        "    embedding_layer = Embedding(vocab_size, embedding_dim, input_length=max_length)(input_layer)\n",
        "    bilstm_layer = Bidirectional(LSTM(64, return_sequences=True))(embedding_layer)\n",
        "    attention_layer = Attention()([bilstm_layer, bilstm_layer])\n",
        "    attention_pooling = GlobalAveragePooling1D()(attention_layer)\n",
        "    dense_layer = Dense(24, activation='relu')(attention_pooling)\n",
        "    output_layer = Dense(1, activation='sigmoid')(dense_layer)\n",
        "    return tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "def create_keras_cnn_model():\n",
        "    return Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        Conv1D(128, 5, activation='relu'),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "def create_glove_cnn_model():\n",
        "    return Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length, trainable=False),\n",
        "        Conv1D(128, 5, activation='relu'),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "def create_glove_bilstm_model():\n",
        "    return Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length, trainable=False),\n",
        "        Bidirectional(LSTM(64)),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "# Function to calculate G-mean\n",
        "def calculate_gmean(conf_matrix):\n",
        "    TN, FP, FN, TP = conf_matrix.ravel()\n",
        "    sensitivity = TP / (TP + FN)  # Recall\n",
        "    specificity = TN / (TN + FP)\n",
        "    gmean = np.sqrt(sensitivity * specificity)\n",
        "    return gmean\n",
        "\n",
        "# Train and evaluate function\n",
        "def train_and_evaluate(model, train_data, train_labels, test_data, test_labels, language, model_name):\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.fit(train_data, train_labels, epochs=10, verbose=2)\n",
        "    predictions = (model.predict(test_data) > 0.5).astype(int)\n",
        "\n",
        "    # Compute metrics\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(test_labels, predictions, average='binary')\n",
        "    macro_f1 = f1_score(test_labels, predictions, average='macro')\n",
        "    accuracy = accuracy_score(test_labels, predictions)\n",
        "    conf_matrix = confusion_matrix(test_labels, predictions)\n",
        "    gmean = calculate_gmean(conf_matrix)\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"{language} - {model_name}:\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1: {f1}\")\n",
        "    print(f\"Macro F1: {macro_f1}\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"G-mean: {gmean}\")\n",
        "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "# Instantiate and evaluate models\n",
        "models = {\n",
        "    'CNN': create_cnn_model(),\n",
        "    'BiLSTM': create_bilstm_model(),\n",
        "    'BiLSTM + Attention': create_bilstm_attention_model(),\n",
        "    'Keras + CNN': create_keras_cnn_model(),\n",
        "    'GloVe + CNN': create_glove_cnn_model(),\n",
        "    'GloVe + BiLSTM': create_glove_bilstm_model()\n",
        "}\n",
        "\n",
        "# Evaluate for Malayalam and Tamil\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nEvaluating {model_name} model for Malayalam:\")\n",
        "    train_and_evaluate(model, mal_train_seq, mal_train_labels, mal_test_seq, mal_test_labels, \"Malayalam\", model_name)\n",
        "    print(f\"\\nEvaluating {model_name} model for Tamil:\")\n",
        "    train_and_evaluate(model, tam_train_seq, tam_train_labels, tam_test_seq, tam_test_labels, \"Tamil\", model_name)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T20:35:43.421123Z",
          "iopub.execute_input": "2025-01-30T20:35:43.421538Z",
          "iopub.status.idle": "2025-01-30T20:41:00.10302Z",
          "shell.execute_reply.started": "2025-01-30T20:35:43.421506Z",
          "shell.execute_reply": "2025-01-30T20:41:00.10206Z"
        },
        "id": "b8VS4iL3JaUz",
        "outputId": "0d40de67-e4ae-4108-8bbe-21cefcc19d93"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nEvaluating CNN model for Malayalam:\nEpoch 1/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "25/25 - 2s - 96ms/step - accuracy: 0.4850 - loss: 0.6947\nEpoch 2/10\n25/25 - 1s - 50ms/step - accuracy: 0.4850 - loss: 0.6935\nEpoch 3/10\n25/25 - 1s - 33ms/step - accuracy: 0.5950 - loss: 0.6882\nEpoch 4/10\n25/25 - 1s - 33ms/step - accuracy: 0.6988 - loss: 0.6562\nEpoch 5/10\n25/25 - 1s - 34ms/step - accuracy: 0.8338 - loss: 0.5383\nEpoch 6/10\n25/25 - 1s - 35ms/step - accuracy: 0.8775 - loss: 0.3600\nEpoch 7/10\n25/25 - 1s - 33ms/step - accuracy: 0.9337 - loss: 0.2070\nEpoch 8/10\n25/25 - 1s - 52ms/step - accuracy: 0.9600 - loss: 0.1233\nEpoch 9/10\n25/25 - 1s - 33ms/step - accuracy: 0.9750 - loss: 0.0761\nEpoch 10/10\n25/25 - 1s - 40ms/step - accuracy: 0.9875 - loss: 0.0471\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\nMalayalam - CNN:\nPrecision: 0.6942148760330579\nRecall: 0.84\nF1: 0.7601809954751132\nMacro F1: 0.7320458050001264\nAccuracy: 0.735\nG-mean: 0.7274613391789284\nConfusion Matrix:\n[[63 37]\n [16 84]]\n==================================================\n\nEvaluating CNN model for Tamil:\nEpoch 1/10\n26/26 - 2s - 86ms/step - accuracy: 0.5309 - loss: 2.2044\nEpoch 2/10\n26/26 - 1s - 33ms/step - accuracy: 0.6411 - loss: 0.6095\nEpoch 3/10\n26/26 - 1s - 48ms/step - accuracy: 0.7748 - loss: 0.5340\nEpoch 4/10\n26/26 - 1s - 54ms/step - accuracy: 0.8342 - loss: 0.4523\nEpoch 5/10\n26/26 - 1s - 41ms/step - accuracy: 0.8874 - loss: 0.3665\nEpoch 6/10\n26/26 - 1s - 38ms/step - accuracy: 0.9196 - loss: 0.2688\nEpoch 7/10\n26/26 - 1s - 34ms/step - accuracy: 0.9418 - loss: 0.1861\nEpoch 8/10\n26/26 - 1s - 57ms/step - accuracy: 0.9715 - loss: 0.1269\nEpoch 9/10\n26/26 - 1s - 51ms/step - accuracy: 0.9802 - loss: 0.0931\nEpoch 10/10\n26/26 - 1s - 36ms/step - accuracy: 0.9827 - loss: 0.0732\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\nTamil - CNN:\nPrecision: 0.6206896551724138\nRecall: 0.375\nF1: 0.4675324675324676\nMacro F1: 0.5670995670995671\nAccuracy: 0.59\nG-mean: 0.5437582890614882\nConfusion Matrix:\n[[41 11]\n [30 18]]\n==================================================\n\nEvaluating BiLSTM model for Malayalam:\nEpoch 1/10\n25/25 - 8s - 321ms/step - accuracy: 0.5738 - loss: 0.6870\nEpoch 2/10\n25/25 - 3s - 116ms/step - accuracy: 0.7812 - loss: 0.5480\nEpoch 3/10\n25/25 - 3s - 123ms/step - accuracy: 0.9337 - loss: 0.2097\nEpoch 4/10\n25/25 - 3s - 116ms/step - accuracy: 0.9237 - loss: 0.2065\nEpoch 5/10\n25/25 - 5s - 208ms/step - accuracy: 0.9900 - loss: 0.0751\nEpoch 6/10\n25/25 - 3s - 120ms/step - accuracy: 0.9987 - loss: 0.0259\nEpoch 7/10\n25/25 - 5s - 208ms/step - accuracy: 1.0000 - loss: 0.0102\nEpoch 8/10\n25/25 - 3s - 122ms/step - accuracy: 1.0000 - loss: 0.0055\nEpoch 9/10\n25/25 - 3s - 112ms/step - accuracy: 1.0000 - loss: 0.0031\nEpoch 10/10\n25/25 - 3s - 109ms/step - accuracy: 1.0000 - loss: 0.0022\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step\nMalayalam - BiLSTM:\nPrecision: 0.8\nRecall: 0.48\nF1: 0.6\nMacro F1: 0.6666666666666667\nAccuracy: 0.68\nG-mean: 0.6499230723708769\nConfusion Matrix:\n[[88 12]\n [52 48]]\n==================================================\n\nEvaluating BiLSTM model for Tamil:\nEpoch 1/10\n26/26 - 7s - 276ms/step - accuracy: 0.7673 - loss: 0.5208\nEpoch 2/10\n26/26 - 3s - 123ms/step - accuracy: 0.9530 - loss: 0.1713\nEpoch 3/10\n26/26 - 3s - 133ms/step - accuracy: 0.9901 - loss: 0.0590\nEpoch 4/10\n26/26 - 3s - 131ms/step - accuracy: 0.9975 - loss: 0.0209\nEpoch 5/10\n26/26 - 3s - 125ms/step - accuracy: 0.9988 - loss: 0.0099\nEpoch 6/10\n26/26 - 6s - 215ms/step - accuracy: 1.0000 - loss: 0.0051\nEpoch 7/10\n26/26 - 3s - 131ms/step - accuracy: 1.0000 - loss: 0.0031\nEpoch 8/10\n26/26 - 3s - 131ms/step - accuracy: 1.0000 - loss: 0.0021\nEpoch 9/10\n26/26 - 5s - 192ms/step - accuracy: 1.0000 - loss: 0.0014\nEpoch 10/10\n26/26 - 3s - 124ms/step - accuracy: 1.0000 - loss: 0.0011\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 143ms/step\nTamil - BiLSTM:\nPrecision: 0.6363636363636364\nRecall: 0.2916666666666667\nF1: 0.4\nMacro F1: 0.5384615384615385\nAccuracy: 0.58\nG-mean: 0.4967845325640401\nConfusion Matrix:\n[[44  8]\n [34 14]]\n==================================================\n\nEvaluating BiLSTM + Attention model for Malayalam:\nEpoch 1/10\n25/25 - 7s - 292ms/step - accuracy: 0.4800 - loss: 0.6947\nEpoch 2/10\n25/25 - 4s - 158ms/step - accuracy: 0.5050 - loss: 0.6922\nEpoch 3/10\n25/25 - 5s - 213ms/step - accuracy: 0.5100 - loss: 0.6956\nEpoch 4/10\n25/25 - 5s - 202ms/step - accuracy: 0.4850 - loss: 0.6983\nEpoch 5/10\n25/25 - 4s - 163ms/step - accuracy: 0.4825 - loss: 0.6942\nEpoch 6/10\n25/25 - 4s - 158ms/step - accuracy: 0.5100 - loss: 0.6932\nEpoch 7/10\n25/25 - 5s - 184ms/step - accuracy: 0.5325 - loss: 0.6914\nEpoch 8/10\n25/25 - 4s - 177ms/step - accuracy: 0.5462 - loss: 0.6900\nEpoch 9/10\n25/25 - 4s - 168ms/step - accuracy: 0.5100 - loss: 0.7093\nEpoch 10/10\n25/25 - 5s - 212ms/step - accuracy: 0.5163 - loss: 0.6960\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step\nMalayalam - BiLSTM + Attention:\nPrecision: 0.5076142131979695\nRecall: 1.0\nF1: 0.6734006734006733\nMacro F1: 0.36582655029256966\nAccuracy: 0.515\nG-mean: 0.17320508075688773\nConfusion Matrix:\n[[  3  97]\n [  0 100]]\n==================================================\n\nEvaluating BiLSTM + Attention model for Tamil:\nEpoch 1/10\n26/26 - 9s - 365ms/step - accuracy: 0.4926 - loss: 0.6959\nEpoch 2/10\n26/26 - 5s - 176ms/step - accuracy: 0.5012 - loss: 0.6948\nEpoch 3/10\n26/26 - 4s - 170ms/step - accuracy: 0.4938 - loss: 0.6937\nEpoch 4/10\n26/26 - 4s - 170ms/step - accuracy: 0.4988 - loss: 0.6921\nEpoch 5/10\n26/26 - 5s - 181ms/step - accuracy: 0.6795 - loss: 0.6362\nEpoch 6/10\n26/26 - 5s - 181ms/step - accuracy: 0.8119 - loss: 0.4436\nEpoch 7/10\n26/26 - 5s - 175ms/step - accuracy: 0.8552 - loss: 0.3630\nEpoch 8/10\n26/26 - 5s - 182ms/step - accuracy: 0.8700 - loss: 0.3399\nEpoch 9/10\n26/26 - 6s - 217ms/step - accuracy: 0.8787 - loss: 0.3236\nEpoch 10/10\n26/26 - 5s - 183ms/step - accuracy: 0.8812 - loss: 0.3186\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step\nTamil - BiLSTM + Attention:\nPrecision: 0.5087719298245614\nRecall: 0.6041666666666666\nF1: 0.5523809523809524\nMacro F1: 0.5288220551378446\nAccuracy: 0.53\nG-mean: 0.5280588545286916\nConfusion Matrix:\n[[24 28]\n [19 29]]\n==================================================\n\nEvaluating Keras + CNN model for Malayalam:\nEpoch 1/10\n25/25 - 3s - 104ms/step - accuracy: 0.4975 - loss: 0.6944\nEpoch 2/10\n25/25 - 1s - 33ms/step - accuracy: 0.4663 - loss: 0.6935\nEpoch 3/10\n25/25 - 1s - 34ms/step - accuracy: 0.6000 - loss: 0.6883\nEpoch 4/10\n25/25 - 1s - 33ms/step - accuracy: 0.7063 - loss: 0.6582\nEpoch 5/10\n25/25 - 1s - 35ms/step - accuracy: 0.8125 - loss: 0.5339\nEpoch 6/10\n25/25 - 2s - 63ms/step - accuracy: 0.8125 - loss: 0.6121\nEpoch 7/10\n25/25 - 1s - 38ms/step - accuracy: 0.8413 - loss: 0.3368\nEpoch 8/10\n25/25 - 1s - 33ms/step - accuracy: 0.9262 - loss: 0.2141\nEpoch 9/10\n25/25 - 1s - 51ms/step - accuracy: 0.9475 - loss: 0.1592\nEpoch 10/10\n25/25 - 1s - 35ms/step - accuracy: 0.9650 - loss: 0.1229\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\nMalayalam - Keras + CNN:\nPrecision: 0.7352941176470589\nRecall: 0.75\nF1: 0.7425742574257426\nMacro F1: 0.7399739973997399\nAccuracy: 0.74\nG-mean: 0.7399324293474371\nConfusion Matrix:\n[[73 27]\n [25 75]]\n==================================================\n\nEvaluating Keras + CNN model for Tamil:\nEpoch 1/10\n26/26 - 2s - 88ms/step - accuracy: 0.5446 - loss: 0.7248\nEpoch 2/10\n26/26 - 1s - 48ms/step - accuracy: 0.7550 - loss: 0.5829\nEpoch 3/10\n26/26 - 1s - 32ms/step - accuracy: 0.8193 - loss: 0.4642\nEpoch 4/10\n26/26 - 1s - 34ms/step - accuracy: 0.9097 - loss: 0.2908\nEpoch 5/10\n26/26 - 1s - 47ms/step - accuracy: 0.9542 - loss: 0.1390\nEpoch 6/10\n26/26 - 1s - 32ms/step - accuracy: 0.9876 - loss: 0.0742\nEpoch 7/10\n26/26 - 1s - 33ms/step - accuracy: 0.9950 - loss: 0.0433\nEpoch 8/10\n26/26 - 1s - 49ms/step - accuracy: 0.9963 - loss: 0.0281\nEpoch 9/10\n26/26 - 1s - 35ms/step - accuracy: 0.9975 - loss: 0.0207\nEpoch 10/10\n26/26 - 1s - 33ms/step - accuracy: 0.9988 - loss: 0.0163\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\nTamil - Keras + CNN:\nPrecision: 0.7714285714285715\nRecall: 0.5625\nF1: 0.6506024096385543\nMacro F1: 0.7013695808876532\nAccuracy: 0.71\nG-mean: 0.6898996582558499\nConfusion Matrix:\n[[44  8]\n [21 27]]\n==================================================\n\nEvaluating GloVe + CNN model for Malayalam:\nEpoch 1/10\n25/25 - 2s - 77ms/step - accuracy: 0.4812 - loss: 0.6938\nEpoch 2/10\n25/25 - 1s - 22ms/step - accuracy: 0.5088 - loss: 0.6921\nEpoch 3/10\n25/25 - 1s - 22ms/step - accuracy: 0.5025 - loss: 0.6927\nEpoch 4/10\n25/25 - 1s - 23ms/step - accuracy: 0.6000 - loss: 0.6888\nEpoch 5/10\n25/25 - 1s - 25ms/step - accuracy: 0.6237 - loss: 0.6843\nEpoch 6/10\n25/25 - 1s - 22ms/step - accuracy: 0.6112 - loss: 0.6802\nEpoch 7/10\n25/25 - 1s - 22ms/step - accuracy: 0.6125 - loss: 0.6731\nEpoch 8/10\n25/25 - 1s - 22ms/step - accuracy: 0.6300 - loss: 0.6610\nEpoch 9/10\n25/25 - 1s - 22ms/step - accuracy: 0.5987 - loss: 0.6597\nEpoch 10/10\n25/25 - 1s - 25ms/step - accuracy: 0.6225 - loss: 0.6516\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\nMalayalam - GloVe + CNN:\nPrecision: 0.636986301369863\nRecall: 0.93\nF1: 0.7560975609756098\nMacro F1: 0.68324358568261\nAccuracy: 0.7\nG-mean: 0.6611353870426238\nConfusion Matrix:\n[[47 53]\n [ 7 93]]\n==================================================\n\nEvaluating GloVe + CNN model for Tamil:\nEpoch 1/10\n26/26 - 2s - 70ms/step - accuracy: 0.4604 - loss: 0.7016\nEpoch 2/10\n26/26 - 1s - 22ms/step - accuracy: 0.4901 - loss: 0.7070\nEpoch 3/10\n26/26 - 1s - 22ms/step - accuracy: 0.5124 - loss: 0.6927\nEpoch 4/10\n26/26 - 1s - 24ms/step - accuracy: 0.5309 - loss: 0.6965\nEpoch 5/10\n26/26 - 1s - 22ms/step - accuracy: 0.4938 - loss: 0.6924\nEpoch 6/10\n26/26 - 1s - 22ms/step - accuracy: 0.5173 - loss: 0.6933\nEpoch 7/10\n26/26 - 1s - 25ms/step - accuracy: 0.5408 - loss: 0.6882\nEpoch 8/10\n26/26 - 1s - 22ms/step - accuracy: 0.5384 - loss: 0.6868\nEpoch 9/10\n26/26 - 1s - 24ms/step - accuracy: 0.5347 - loss: 0.6865\nEpoch 10/10\n26/26 - 1s - 23ms/step - accuracy: 0.5619 - loss: 0.6833\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\nTamil - GloVe + CNN:\nPrecision: 0.34615384615384615\nRecall: 0.5625\nF1: 0.4285714285714286\nMacro F1: 0.22779922779922782\nAccuracy: 0.28\nG-mean: 0.10400628679223047\nConfusion Matrix:\n[[ 1 51]\n [21 27]]\n==================================================\n\nEvaluating GloVe + BiLSTM model for Malayalam:\nEpoch 1/10\n25/25 - 6s - 233ms/step - accuracy: 0.5138 - loss: 0.6922\nEpoch 2/10\n25/25 - 2s - 93ms/step - accuracy: 0.6275 - loss: 0.6731\nEpoch 3/10\n25/25 - 4s - 146ms/step - accuracy: 0.6162 - loss: 0.6313\nEpoch 4/10\n25/25 - 3s - 124ms/step - accuracy: 0.6587 - loss: 0.5950\nEpoch 5/10\n25/25 - 2s - 93ms/step - accuracy: 0.6737 - loss: 0.5729\nEpoch 6/10\n25/25 - 2s - 97ms/step - accuracy: 0.6975 - loss: 0.5580\nEpoch 7/10\n25/25 - 2s - 96ms/step - accuracy: 0.6963 - loss: 0.5574\nEpoch 8/10\n25/25 - 3s - 112ms/step - accuracy: 0.6913 - loss: 0.5573\nEpoch 9/10\n25/25 - 2s - 91ms/step - accuracy: 0.7013 - loss: 0.5499\nEpoch 10/10\n25/25 - 2s - 98ms/step - accuracy: 0.7088 - loss: 0.5376\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step\nMalayalam - GloVe + BiLSTM:\nPrecision: 0.74\nRecall: 0.74\nF1: 0.74\nMacro F1: 0.74\nAccuracy: 0.74\nG-mean: 0.74\nConfusion Matrix:\n[[74 26]\n [26 74]]\n==================================================\n\nEvaluating GloVe + BiLSTM model for Tamil:\nEpoch 1/10\n26/26 - 7s - 261ms/step - accuracy: 0.5384 - loss: 0.6812\nEpoch 2/10\n26/26 - 3s - 114ms/step - accuracy: 0.6423 - loss: 0.6534\nEpoch 3/10\n26/26 - 3s - 111ms/step - accuracy: 0.6473 - loss: 0.6430\nEpoch 4/10\n26/26 - 3s - 103ms/step - accuracy: 0.5842 - loss: 0.7233\nEpoch 5/10\n26/26 - 3s - 106ms/step - accuracy: 0.5854 - loss: 0.6554\nEpoch 6/10\n26/26 - 5s - 195ms/step - accuracy: 0.6795 - loss: 0.6367\nEpoch 7/10\n26/26 - 3s - 106ms/step - accuracy: 0.6795 - loss: 0.6249\nEpoch 8/10\n26/26 - 5s - 192ms/step - accuracy: 0.6906 - loss: 0.6093\nEpoch 9/10\n26/26 - 3s - 117ms/step - accuracy: 0.7030 - loss: 0.6009\nEpoch 10/10\n26/26 - 3s - 108ms/step - accuracy: 0.6955 - loss: 0.5992\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 331ms/step\nTamil - GloVe + BiLSTM:\nPrecision: 0.6666666666666666\nRecall: 0.25\nF1: 0.36363636363636365\nMacro F1: 0.525101763907734\nAccuracy: 0.58\nG-mean: 0.4702699715629801\nConfusion Matrix:\n[[46  6]\n [36 12]]\n==================================================\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer for mala tam"
      ],
      "metadata": {
        "id": "NdlBYQG2JaU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-25T18:58:59.67069Z",
          "iopub.execute_input": "2025-01-25T18:58:59.671204Z",
          "iopub.status.idle": "2025-01-25T18:59:05.67466Z",
          "shell.execute_reply.started": "2025-01-25T18:58:59.671167Z",
          "shell.execute_reply": "2025-01-25T18:59:05.672995Z"
        },
        "id": "lyeBzfKtJaU0",
        "outputId": "5b597dca-440f-440b-a11e-177856695cd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Function to prepare datasets for training\n",
        "def prepare_datasets(texts, labels):\n",
        "    return texts.tolist(), np.array(labels)\n",
        "\n",
        "# Load datasets\n",
        "mal_train = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/mal_training_data_hum_ai.csv')\n",
        "mal_test = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/mal_test.xlsx - Sheet1.csv')\n",
        "tam_train = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/tam_training_data_hum_ai.csv')\n",
        "tam_test = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/tamil-test.xlsx - Sheet1.csv')\n",
        "\n",
        "\n",
        "#mal_train = pd.read_csv('/kaggle/input/final-dataset/mal_training_data_hum_ai.csv')\n",
        "#mal_test = pd.read_excel('/kaggle/input/final-dataset/mal_test.xlsx')\n",
        "#tam_train = pd.read_csv('/kaggle/input/final-dataset/tam_training_data_hum_ai.csv')\n",
        "#tam_test = pd.read_excel('/kaggle/input/final-dataset/tamil-test.xlsx')\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "mal_train_labels = label_encoder.fit_transform(mal_train['LABEL'])\n",
        "mal_test_labels = label_encoder.transform(mal_test['Label'])\n",
        "tam_train_labels = label_encoder.fit_transform(tam_train['LABEL'])\n",
        "tam_test_labels = label_encoder.transform(tam_test['Label'])\n",
        "\n",
        "# Tokenization and model preparation\n",
        "def train_and_evaluate_transformer(model, tokenizer, train_texts, train_labels, test_texts, test_labels, language, model_name):\n",
        "    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
        "    test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels)).batch(16)\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels)).batch(16)\n",
        "\n",
        "    model.compile(optimizer='adam', loss=model.compute_loss, metrics=['accuracy'])\n",
        "    print(f\"Training {model_name} for {language}...\")\n",
        "    model.fit(train_dataset, epochs=3)\n",
        "    print(f\"Evaluating {model_name} for {language}...\")\n",
        "    eval_results = model.evaluate(test_dataset)\n",
        "    print(f\"{language} - {model_name} - Loss: {eval_results[0]}, Accuracy: {eval_results[1]}\")\n",
        "\n",
        "# Initialize models and tokenizers\n",
        "models_info = {\n",
        "    'DistilBERT': ('distilbert-base-uncased', AutoTokenizer, TFAutoModelForSequenceClassification),\n",
        "    'MuRIL': ('google/muril-base-cased', AutoTokenizer, TFAutoModelForSequenceClassification),\n",
        "    'RoBERTa': ('roberta-base', AutoTokenizer, TFAutoModelForSequenceClassification)\n",
        "}\n",
        "\n",
        "for model_name, (model_path, tokenizer_class, model_class) in models_info.items():\n",
        "    tokenizer = tokenizer_class.from_pretrained(model_path)\n",
        "    model = model_class.from_pretrained(model_path, num_labels=len(label_encoder.classes_))\n",
        "    print(f\"\\nProcessing {model_name}...\")\n",
        "\n",
        "    # Training and evaluating for Malayalam\n",
        "    mal_train_texts, mal_train_labels = prepare_datasets(mal_train['DATA'], mal_train_labels)\n",
        "    mal_test_texts, mal_test_labels = prepare_datasets(mal_test['DATA'], mal_test_labels)\n",
        "    train_and_evaluate_transformer(model, tokenizer, mal_train_texts, mal_train_labels, mal_test_texts, mal_test_labels, \"Malayalam\", model_name)\n",
        "\n",
        "    # Training and evaluating for Tamil\n",
        "    tam_train_texts, tam_train_labels = prepare_datasets(tam_train['DATA'], tam_train_labels)\n",
        "    tam_test_texts, tam_test_labels = prepare_datasets(tam_test['DATA'], tam_test_labels)\n",
        "    train_and_evaluate_transformer(model, tokenizer, tam_train_texts, tam_train_labels, tam_test_texts, tam_test_labels, \"Tamil\", model_name)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "RUAifHbmJaU0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "mal_train = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/mal_training_data_hum_ai.csv')\n",
        "mal_test = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/mal_test.xlsx - Sheet1.csv')\n",
        "tam_train = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/tam_training_data_hum_ai.csv')\n",
        "tam_test = pd.read_csv('/kaggle/input/ai-review-ml-dl-transformer/tamil-test.xlsx - Sheet1.csv')\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "YMDqvgUEJaU1"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}